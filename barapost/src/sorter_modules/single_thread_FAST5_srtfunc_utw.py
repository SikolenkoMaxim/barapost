# -*- coding: utf-8 -*-

import h5py

import os
import sys
from glob import glob

from src.sorter_modules.sorter_spec import *
from src.sorter_modules.fast5 import update_file_dict
from src.sorter_modules.fast5 import fast5_readids, copy_read_f5_2_f5, copy_single_f5
from src.sorter_modules.filters import get_filter, get_trash_fpath

from src.platform import platf_depend_exit
from src.printlog import printl, printn, getwt, err_fmt
from src.filesystem import get_curr_res_dpath, is_fastq
from src.fmt_readID import fmt_read_id

from shelve import open as open_shelve

index_name = "fast5_to_tsvtaxann_idx"


def sort_fast5_file(f5_path, tax_annot_res_dir, sens,
        min_qual, min_qlen, min_pident, min_coverage, logfile_path):
    """
    Function sorts FAST5 file with untwisting.

    :param f5_path: path to FAST5 file meant to be processed;
    :type f5_path: str;
    :param tax_annot_res_dir: path to directory containing taxonomic annotation;
    :type tax_annot_res_dir: str;
    :param sens: sorting sensitivity;
    :type sens: str;
    :param min_qual: threshold for quality filter;
    :type min_qual: float;
    :param min_qlen: threshold for length filter;
    :type min_qlen: int (or None, if this filter is disabled);
    :param min_pident: threshold for alignment identity filter;
    :type min_pident: float (or None, if this filter is disabled);
    :param min_coverage: threshold for alignment coverage filter;
    :type min_coverage: float (or None, if this filter is disabled);
    :param logfile_path: path to log file;
    :type logfile_path: str;
    """

    outdir_path = os.path.dirname(logfile_path)

    seqs_pass = 0
    seqs_fail = 0
    srt_file_dict = dict()

    index_dirpath = os.path.join(tax_annot_res_dir, index_name) # name of directory that will contain indicies

    # Make filter
    seq_filter = get_filter(f5_path, min_qual, min_qlen, min_pident, min_coverage)
    # Configure path to trash file
    trash_fpath = get_trash_fpath(f5_path, outdir_path, min_qual, min_qlen, min_pident, min_coverage)

    from_f5 = h5py.File(f5_path, 'r')

    num_reads = len(from_f5) # get number of reads in it

    # singleFAST5 and multiFAST5 files should be processed in different ways
    # "Raw" group always in singleFAST5 root and never in multiFAST5 root
    if "Raw" in from_f5.keys():
        f5_cpy_func = copy_single_f5
    else:
        f5_cpy_func = copy_read_f5_2_f5
    # end if

    readids_to_seek = list(from_f5.keys()) # list of not-sorted-yet read IDs

    # Fill the list 'readids_to_seek'
    for read_name in fast5_readids(from_f5):
        # Get rid of "read_"
        readids_to_seek.append(sys.intern(read_name))
    # end for

    # Walk through the index
    index_f5_2_tsv = open_shelve( os.path.join(index_dirpath, index_name), 'r' )

    if not f5_path in index_f5_2_tsv.keys():
        printl(logfile_path, err_fmt("Source FAST5 file not found in index"))
        printl(logfile_path, "Try to rebuild index")
        platf_depend_exit(1)
    # end if

    for tsv_path in index_f5_2_tsv[f5_path].keys():

        read_names = index_f5_2_tsv[f5_path][tsv_path]
        resfile_lines = configure_resfile_lines(tsv_path, sens)

        for read_name in read_names:
            try:
                hit_names, *vals_to_filter = resfile_lines[sys.intern(fmt_read_id(read_name)[1:])]
            except KeyError:
                printl(logfile_path,
                    err_fmt("missing taxonomic annotation info for read '{}'".format(fmt_read_id(read_name)[1:])))
                printl(logfile_path, "It is stored in '{}' FAST5 file".format(f5_path))
                printl(logfile_path, "Try to make new index file (press ENTER on corresponding prompt).")
                printl(logfile_path, """Or, if does not work for you, make sure that taxonomic annotation info
for this read is present in one of TSV files generated by 'prober.py' and 'barapost.py'.""")
                index_f5_2_tsv.close()
                platf_depend_exit(1)
            else:
                if seq_filter(vals_to_filter):
                    for hit_name in hit_names.split("&&"): # there can be multiple hits for single query sequence
                        # Get name of result FASTQ file to write this read in
                        sorted_file_path = os.path.join(outdir_path, "{}.fast5".format(hit_name))
                        if sorted_file_path not in srt_file_dict.keys():
                            srt_file_dict = update_file_dict(srt_file_dict, sorted_file_path, logfile_path)
                        # end if
                        f5_cpy_func(from_f5, read_name, srt_file_dict[sorted_file_path], logfile_path)
                    # end for
                    seqs_pass += 1
                else:
                    # Get name of result FASTQ file to write this read in
                    if trash_fpath not in srt_file_dict.keys():
                        srt_file_dict = update_file_dict(srt_file_dict, trash_fpath, logfile_path)
                    # end if
                    f5_cpy_func(from_f5, read_name, srt_file_dict[trash_fpath], logfile_path)
                    seqs_fail += 1
                # end if
            # end try
        # end for

    index_f5_2_tsv.close()
    # Close all sorted files
    for file_obj in srt_file_dict.values():
        file_obj.close()
    # end for

    printl(logfile_path, "\r{} - File '{}' is sorted.".format(getwt(), os.path.basename(f5_path)))
    printn(" Working...")

    return (seqs_pass, seqs_fail)
# end def sort_fast5_file
