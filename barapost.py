#!/usr/bin/env python3
# -*- coding: utf-8 -*-

__version__ = "3.6.c"
# Year, month, day
__last_update_date__ = "2019-12-07"

# |===== Check python interpreter version =====|

from sys import version_info as verinf

if verinf.major < 3:
    print( "\nYour python interpreter version is " + "%d.%d" % (verinf.major, verinf.minor) )
    print("   Please, use Python 3!\a")
    # In python 2 'raw_input' does the same thing as 'input' in python 3.
    # Neither does 'input' in python2.
    raw_input("Press ENTER to exit:")
    exit(1)
# end if

from sys import platform

def platf_depend_exit(exit_code):
    """
    Function asks to press ENTER press on Windows
        and exits after that.

    :type exit_code: int;
    """
    if platform.startswith("win"):
        input("Press ENTER to exit:")
    # end if
    exit(exit_code)
# end def platf_depend_exit

from sys import argv

# First search for information-providing options:

if "-h" in argv[1:] or "--help" in argv[1:]:
    print("\n  barapost.py\n  Version {}; {} edition;\n".format(__version__, __last_update_date__))
    print("DESCRIPTION:\n")
    print("""This script is designed for taxonomic annotation of nucleotide sequences by "BLASTing"
  each of them with 'blastn' script from "BLAST+" toolkit and regarding the best hit.\n""")
    print("\"barapost.py\" is meant to be used just after 'prober.py'.\n")

    if "--help" in argv[1:]:
        print("""\"barapost.py\" downloads records-hits from Genbank according to results (`...probe_acc_list.tsv`)
  generated by "prober.py", builds a database (on your local machine) which consists of downloaded
  sequences, and continues aligning the rest of your with "BLAST+" toolkit.\n""")
        print("Script processes FASTQ and FASTA (as well as '.fastq.gz' and '.fasta.gz') files.\n")
        print("\"barapost.py\" writes (actually appends) it's results in the same TSV file as \"prober.py\" does.\n")
        print("Files processed by this script are meant to be sorted afterwards by 'fastQA5_sorter.py'.\n")
        print("""If you have your own FASTA files that can be used as database to blast against,
  you can omit "prober.py" step and go to "barapost.py" (see `-l` option).""")
        print("----------------------------------------------------------\n")
        print("Default parameters:\n")
        print("- all FASTQ and FASTA files in current directory will be processed;")
        print("- packet size (see '-p' option): 100 sequences;")
        print("- algorithm (see '-a' option): 'megaBlast';")
        print("- numbers of threads to launch ('-t' option): 1 thread.")
        print("----------------------------------------------------------\n")
    # end if

    print("""Files that you want 'barapost.py' to process should be specified as
  positional arguments (see EXAMPLE #2 running detailed (--help) help message).
  Wildcards do work: './barapost.py my_directory/*'' will process all files in 'my_directory'.""")
    print("----------------------------------------------------------\n")
    print("OPTIONS:\n")
    print("""-h (--help) --- show help message.
        '-h' -- brief, '--help' -- full;\n""")
    print("-v (--version) --- show version;\n")
    print("""-d (--indir) --- directory which contains FASTQ of FASTA files meant to be processed.
        I.e. all FASTQ and FASTA files in this direcory will be processed;
        Input files can be gzipped.\n""")
    print("""-p (--packet-size) --- size of the packet, i.e. number of sequence to blast in one request.
        Value: integer number [1, 500]. Default value is 100;\n""")
    print("""-a (--algorithm) --- BLASTn algorithm to use for aligning.
        Available values: 'megaBlast', 'discoMegablast', 'blastn'.
        Default is megaBlast;\n""")
    print("""-r (--taxannot-resdir) --- result directory generated by script 'prober.py'
        This is directory specified to 'prober.py' by '-o' option.
        If you omit 'prober.py' and use your own FASTA files
        to create a database, this directory may not exist before start of 'barapost.py'
        (i.e. it will be a simple output directory).
        Default value is "barapost_result", since it is the default name of
        output directory generated by "prober.py".\n""")
    print("""-l (--local-fasta-to-db) --- your own FASTA file that will be added to downloaded database
        or used instead of it if you omit 'prober.py' step;\n""")
    print("-t (--threads) --- number of threads to launch;")

    if "--help" in argv[1:]:
        print("----------------------------------------------------------\n")
        print("EXAMPLES:\n")
        print("""1. Process all FASTA and FASTQ files in working directory with default settings:\n
  ./barapost.py\n""")
        print("""2. Process all files in the working directory that start with "some_my_fasta". Use default settings:\n
  ./barapost.py some_my_fasta*\n""")
        print("""3. Process one FASTQ file with default settings.
  File 'reads.fastq' has been already processed by "prober.py".
  Results of "prober.py" work are in directory 'prober_outdir':\n
  ./barapost.py reads.fastq -r prober_outdir\n""")
        print("""4. Process FASTQ file and FASTA file with discoMegablast, packet size of 100 sequences.
  Files 'reads.fastq.gz' and 'another_sequences.fasta' have been already processed by "prober.py".
  Results of "prober.py" work are in directory 'prober_outdir':\n
  ./barapost.py reads.fastq.gz another_sequences.fasta -a discoMegablast -p 100 -r prober_outdir\n""")
        print("""5. Process all FASTQ and FASTA files in directory named 'some_dir'.
  All these files have been already processed by "prober.py".
  Results of "prober.py" work are in directory 'prober_outdir':\n
  ./barapost.py -d some_dir -r prober_outdir\n""")
        print("""6. Process file named 'some_reads.fastq'. This file has been already processed by "prober.py".
  Results of "prober.py" work are in directory 'prober_outdir'.
  Sequence from file 'my_own_sequence.fasta' will be included to the database.
  Packet size is 50 sequences. Launch 4 threads.\n
  ./barapost.py some_reads.fastq -p 50 -l my_own_sequence.fasta -t 4 -r prober_outdir""")
     # end if
    platf_depend_exit(0)
# end if

if "-v" in argv[1:] or "--version" in argv[1:]:
    print(__version__)
    platf_depend_exit(0)
# end if

# |===== Stuff for dealing with time =====|

from time import time, strftime, gmtime, sleep, localtime
start_time = time()

def getwt():
    return strftime("%H:%M:%S", gmtime(time() - start_time))
# end def getwt

# |===========================================|


def err_fmt(text):
    """Function for configuring error messages"""
    return "\n   \a!! - ERROR: " + text + '\n'
# end def print_error

from sys import stdout as sys_stdout
def printn(text):
    """
    Function prints text to the console without adding '\n' in the end of the line.
    Why not just to use 'print(text, end="")'?
    In order to display informative error message if Python 2.X is launched
        instead if awful error traceback.
    """
    sys_stdout.write(text)
    sys_stdout.flush()
# end def printn


import os
import multiprocessing as mp
from re import search as re_search
from glob import glob
import getopt

try:
    opts, args = getopt.gnu_getopt(argv[1:], "hvd:p:a:r:l:t:",
        ["help", "version", "indir=", "packet-size=", "algorithm=", "taxannot-resdir=",
        "local-fasta-to-bd=", "threads="])
except getopt.GetoptError as gerr:
    print( str(gerr) )
    platf_depend_exit(2)
# end try

is_fq_or_fa = lambda f: True if not re_search(r".*\.(m)?f(ast)?(a|q)(\.gz)?$", f) is None else False

# Default values:
fq_fa_list = list()
indir_path = None
packet_size = 100 # default
blast_algorithm = "megaBlast" # default
tax_annot_res_dir = "barapost_result" # default
your_own_fasta_lst = list()
n_thr = 1

# Add positional arguments to fq_fa_list
for arg in args:
    if not os.path.exists(arg) or not is_fq_or_fa(arg):
        print(err_fmt("invalid positional argument: '{}'".format(arg)))
        print("Only FAST(A/Q) files can be specified without an option in command line.")
        platf_depend_exit(1)
    # end if
    fq_fa_list.append( os.path.abspath(arg) )
# end for

for opt, arg in opts:

    if opt in ("-d", "--indir"):
        if not os.path.exists(arg):
            print(err_fmt("directory '{}' does not exist!".format(arg)))
            platf_depend_exit(1)
        # end if
        
        if not os.path.isdir(arg):
            print(err_fmt("'{}' is not a directory!".format(arg)))
            platf_depend_exit(1)
        # end if
        indir_path = os.path.abspath(arg)

        fq_fa_list.extend(list( filter(is_fq_or_fa, glob("{}{}*".format(indir_path, os.sep))) ))

    elif opt in ("-p", "--packet-size"):
        try:
            packet_size = int(arg)
            if packet_size < 1 or packet_size > 500:
                raise ValueError
            # end if
        except ValueError:
            print(err_fmt("packet_size (-p option) must be integer number from 1 to 500"))
            platf_depend_exit(1)
        # end try

    elif opt in ("-a", "--algorithm"):
        if not arg in ("megaBlast", "discoMegablast", "blastn"):
            print(err_fmt("invalid value specified by '-a' option!"))
            print("Available values: 'megaBlast', 'discoMegablast', 'blastn'")
            platf_depend_exit(1)
        # end if
        
        blast_algorithm = arg

    elif opt in ("-r", "--taxannot-resdir"):
        tax_annot_res_dir = os.path.abspath(arg)
    # end if

    elif opt in ("-l", "--local-fasta-to-bd"):

        if not os.path.exists(arg):
            print(err_fmt("file '{}' does not exist!".format(arg)))
            platf_depend_exit(1)
        # end if

        your_own_fasta_lst.append(os.path.abspath(arg))

    elif opt in ("-t", "--threads"):
        try:
            n_thr = int(arg)
            if n_thr < 1:
                raise ValueError
            # end if
        except ValueError:
            print(err_fmt("number of threads must be positive integer number!"))
            print(" And here is your value: '{}'".format(arg))
            exit(1)
        # end try
        if n_thr > mp.cpu_count():
            print("""\nWarning! You have specified {} threads to use
  although {} are available.""".format(n_thr, mp.cpu_count()))
            error = True
            while error:
                reply = input("""\nPress ENTER to switch to {} threads,
  or enter 'c' to continue with {} threads,
  or enter 'q' to exit:>>""".format(mp.cpu_count(), n_thr))
                if reply in ("", 'c', 'q'):
                    error = False
                    if reply == "":
                        n_thr = mp.cpu_count()
                        print("\nNumber of threads switched to {}\n".format(n_thr))
                    elif reply == 'c':
                        pass
                    elif reply == 'q':
                        exit(0)
                    # end if
                else:
                    print("\nInvalid reply!\n")
                # end if
            # end while
        # end if
    # end if
# end for


# If no FASTQ or FASTA file have been found
if len(fq_fa_list) == 0:
    # If input directory was specified -- exit
    if not indir_path is None:
        print(err_fmt("""no input FASTQ or FASTA files specified
    or there is no FASTQ and FASTA files in the input directory.\n"""))
        platf_depend_exit(1)
    
    # If input directory was not specified -- look for FASTQ files in current directory
    else:
        fq_fa_list = list(filter( is_fq_or_fa, glob("{}{}*".format(os.getcwd(), os.sep)) ))
        if len(fq_fa_list) == 0:
            print(err_fmt("there are no FASTQ or FASTA files found to process."))
            platf_depend_exit(1)
        # end if
    # end if
# end if

fq_fa_list.sort()


# Check if 'blast+' tookit is installed
pathdirs = os.environ["PATH"].split(os.pathsep)

# Add '.exe' extention in order to find executables on Windows
if platform.startswith("win"):
    exe_ext = ".exe"
else:
    exe_ext = ""
# end if

for utility in ("blastn"+exe_ext, "makeblastdb"+exe_ext, "makembindex"+exe_ext):

    utility_found = False

    for directory in pathdirs:
        if os.path.exists(directory) and utility in os.listdir(directory):
            utility_found = True
            break
        # end if
    # end for

    if not utility_found:
        print("\tAttention!\n'{}' from blast+ toolkit is not found in your system.".format(utility))
        print("""If this error still occures although you have installed everything 
-- make sure that this program is added to PATH)""")
        platf_depend_exit(1)
    # end if
# end for

acc_fpath = None
is_acc_file = lambda f: False if re_search(r".*probe_acc_list.tsv", f) is None else True

try:
    seem_like_acc_files = filter(is_acc_file, os.listdir(tax_annot_res_dir))
except FileNotFoundError:
    # If 'tax_annot_res_dir' does not exist-- obviously there is no accession file
    like_acc_num = 0
else:
    like_acc_num = len(list(seem_like_acc_files))
# end try

db_exists = os.path.exists( os.path.join(tax_annot_res_dir, "local_database") )
if db_exists:
    db_exists = db_exists and len(os.listdir(os.path.join(tax_annot_res_dir, "local_database"))) != 0
# end if

if like_acc_num == 1:
    acc_fpath = os.path.join(tax_annot_res_dir, next(filter(is_acc_file, os.listdir(tax_annot_res_dir)))) # form path to accession file
elif like_acc_num == 0 and len(your_own_fasta_lst) != 0:
    if not os.path.exists(tax_annot_res_dir):
        try:
            os.makedirs(tax_annot_res_dir)
        except OSError as oserr:
            print(err_fmt("unable to create result directory"))
            print( str(oserr) )
            print("Prober just tried to create directory '{}' and crushed.".format(tax_annot_res_dir))
            platf_depend_exit(1)
        # end try
    # end if
elif like_acc_num == 0 and len(your_own_fasta_lst) == 0 and db_exists:
    pass
elif like_acc_num > 1:
    printl(err_fmt("multiple files with accession:"))
    for f in filter(is_acc_file, os.listdir(tax_annot_res_dir)):
        printl("  '{}'".format(f))
    # end for
    print("There should be only one accession file.")
    print("Please, remove odd files and leave only one of them.")
    platf_depend_exit(1)
elif like_acc_num == 0 and len(your_own_fasta_lst) == 0 and not db_exists:

    print(err_fmt("no way to build a database '{}'!".format(tax_annot_res_dir)))
    if like_acc_num == 0:
        print("There is no accession file in directory '{}'".format(tax_annot_res_dir))
    # end if
    if tax_annot_res_dir == "barapost_result":
        print("""Maybe, output directory generated by 'prober.py' hasn't been named 'barapost_result'
    and you have forgotten to specify it with '-r' option.""")
    # end if
    platf_depend_exit(1)
else:
    # Execution should not reach here
    print(err_fmt("fatal error. Please, contact the developer."))
    platf_depend_exit(1)
# end if
del db_exists

from subprocess import Popen as sp_Popen, PIPE as sp_PIPE
from shutil import get_terminal_size
from gzip import open as open_as_gzip # input files might be gzipped
from xml.etree import ElementTree # for retrieving information from XML BLAST report
from sys import intern

import http.client
import urllib.request
from urllib.error import HTTPError
import urllib.parse
import socket


# There some troubles with file extention on Windows, so let's make a .txt file for it:
log_ext = ".log" if not platform.startswith("win") else ".txt"
logfile_path = os.path.join(tax_annot_res_dir, "barapost_log_{}{}".format(strftime("%Y-%m-%d_%H-%M-%S", localtime(start_time)), log_ext))
logfile = open(logfile_path, 'w')

def printl(text=""):
    """
    Function for printing text to console and to log file.
    """
    print(text)
    logfile.write(str(text).strip('\r') + '\n')
    logfile.flush()
# end def printl

def println(text=""):
    """
    Function for printing text to console and to log file.
    The only difference from 'printl' -- text that is printed to console does not end with '\\n'
    """
    printn(text)
    logfile.write(str(text).strip('\r') + '\n')
    logfile.flush()
# end def printl

printl("\n |=== barapost.py (version {}) ===|\n".format(__version__))
printl( getwt() + " ({}) ".format(strftime("%Y-%m-%d %H:%M:%S", localtime(start_time))) + "- Start working\n")


# |===== Function for checking if 'https://ncbi.nlm.nih.gov' is available =====|

def check_connection():
    """
    Function checks if 'https://ncbi.nlm.nih.gov' is available.
    """
    printn("Checking Internet connection...")

    if not platform.startswith("win"):
        check_mark = "\u2714"
    else:
        check_mark = "OK"
    # end if

    try:
        ncbi_server = "https://ncbi.nlm.nih.gov"
        status_code = urllib.request.urlopen(ncbi_server).getcode()
        # Just in case
        if status_code != 200:
            printl('\n' + getwt() + " - Site '{}' is not available.".format(ncbi_server))
            print("Check your Internet connection.\a")
            printl("Status code: {}".format(status_code))
            platf_depend_exit(-2)
        # end if
    except OSError as err:

        printl('\n' + getwt() + " - Site '{}' is not available.".format(ncbi_server))
        print("Check your Internet connection.\a")
        printl( str(err) )

        # 'urllib.request.HTTPError' can provide a user with information about the error
        if isinstance(err, HTTPError):
            printl("Status code: {}".format(err.code))
            printl(err.reason)
        # end if
        platf_depend_exit(-2)
    else:
        print("\rChecking Internet connection... {}\n".format(check_mark))
    # end try
# end def check_connection


# |===== Functionality for proper processing of gzipped files =====|

OPEN_FUNCS = (open, open_as_gzip)

is_gzipped = lambda file: True if file.endswith(".gz") else False
is_fastq = lambda f: True if not re_search(r".*\.fastq(\.gz)?$", f) is None else False

# Data from plain text and gzipped should be parsed in different way,
#   because data from .gz is read as 'bytes', not 'str'.
FORMATTING_FUNCS = (
    lambda line: line.strip(),   # format text line
    lambda line: line.decode("utf-8").strip()  # format gzipped line
)

# |=== Delimiter for result tsv file ===|
DELIM = '\t'

# |=== File format constants ===|
FASTQ_LINES_PER_READ = 4
FASTA_LINES_PER_SEQ = 2


from shutil import copyfileobj as shutil_copyfileobj

def fastq2fasta(fq_fa_path, new_dpath):
    """
    Function conwerts FASTQ file to FASTA format, if there is no FASTA file with
        the same name as FASTQ file. Additionally it counts sequences in this file.

    :param fq_fa_path: path to FASTQ or FASTA file being processed;
    :type fq_fa_path: str;
    :param new_dpath: path to current (corresponding to fq_fa_path file) result directory;
    :type new_dpath: str;

    Returns dict of the following structure:
    {
        "fpath": path_to_FASTA_file (str),
        "nreads": number_of_reads_in_this_FASTA_file (int)
    }
    """
    
    fasta_path = re_search(r"(.*)\.(m)?f(ast)?(a|q)", os.path.basename(fq_fa_path)).group(1) + ".fasta"
    fasta_path = os.path.join(new_dpath, fasta_path) # place FASTA file into result directory

    how_to_open = OPEN_FUNCS[ is_gzipped(fq_fa_path) ]
    fmt_func = FORMATTING_FUNCS[ is_gzipped(fq_fa_path) ]

    fastq_patt = r".*\.f(ast)?q(\.gz)?$"

    num_lines = 0 # variable for counting lines in a file
    if not re_search(fastq_patt, fq_fa_path) is None and not os.path.exists(fasta_path+".gz"):

        with how_to_open(fq_fa_path) as fastq_file, open(fasta_path, 'w') as fasta_file:

            counter = 1 # variable for retrieving only 1-st and 2-nd line of FASTQ record
            for line in fastq_file:
                line = fmt_func(line)
                # write only 1-st and 2-nd line out of 4
                if counter <= 2:
                    if line[0] == '@':
                        line = '>' + line[1:]  # replace '@' with '>'
                    # end if
                    fasta_file.write(line + '\n')
                
                # reset the counter if the 4-th (quality) line has been encountered
                elif counter == 4:
                    counter = 0
                # end if
                counter += 1
                num_lines += 1
            # end for
        # end with
        num_reads = int(num_lines / FASTQ_LINES_PER_READ) # get number of sequences

        # GNU gzip utility is faster, but there can be presence of absence of it
        gzip_util = "gzip"
        util_found = False
        for directory in os.environ["PATH"].split(os.pathsep):
            if os.path.isdir(directory) and gzip_util in os.listdir(directory):
                util_found = True
                break
            # end if
        # end for

        if util_found:
            os.system("{} {}".format(gzip_util, fasta_path))
        else:
            # form .fasta.gz file 'by hand'
            with open(fasta_path, 'rb') as fasta_file, open_as_gzip(fasta_path+".gz", "wb") as fagz_file:
                shutil_copyfileobj(fasta_file, fagz_file)
            # end with
            os.unlink(fasta_path) # remove plain FASTA file
        # end if
    
    # IF FASTA file is already created
    # We need only number of sequences in it.
    elif not re_search(fastq_patt, fq_fa_path) is None and os.path.exists(fasta_path+".gz"):
        num_lines = sum(1 for line in how_to_open(fq_fa_path)) # get number of lines
        num_reads = int( num_lines / FASTQ_LINES_PER_READ ) # get number of sequences
    
    # If we've got FASTA source file
    # We need only number of sequences in it.
    else:
        num_reads = sum(1 for line in how_to_open(fq_fa_path) if fmt_func(line)[0] == '>')
        # If we've got FASTA source file we do not need to copy it
        fasta_path = fq_fa_path
        return {"fpath": fasta_path, "nreads": num_reads}
    # end if

    return {"fpath": fasta_path+".gz", "nreads": num_reads}
# end def fastq2fasta


def rename_file_verbosely(file, pardir):
    """
    Function verbosely renames file (as well as directory) given to it.

    :param file: path to file (directory) meant to be renamed;
    :type file: str;
    :param pardir: parent directoy of file';
    :type pardir: str;
    """
    
    # Directory is a file, so let's rename it too.
    if os.path.isdir(file):
        # Count files in 'pardir' that have analogous names as 'file' has:
        is_analog = lambda f: file in f
        word = "directory"
    else:
        # Count files in 'pardir' that have analogous names as 'file' has:
        is_analog = lambda f: re_search(r"(.*)\..*$", os.path.basename(file)).group(1) in f
        word = "file"
    # end if

    num_analog_files = len( list(filter(is_analog, os.listdir(pardir))) )

    try:
        printl('\n' + getwt() + " - Renaming old {}:".format(word))
        if not os.path.isdir(file):
            name_itself = re_search(r"(.*)\..*$", file).group(1)
            ext = re_search(r".*\.(.*)$", file).group(1)
        else:
            name_itself = file
            ext = ""
        # end if
        num_analog_files = str(num_analog_files)
        new_name = name_itself+"_old_"+num_analog_files+ext
        printl("  '{}' --> '{}'".format(file, new_name))
        os.rename(file, new_name)
    except Exception as err:
        # Anything (and not only strings) can be passed to the function
        printl("\n {} '{}' cannot be renamed:".format( word, str(file)) )
        printl( str(err) + '\n')
        platf_depend_exit(1)
    # end try
# end def rename_file_verbosely


def look_around(new_dpath, fasta_path, blast_algorithm):
    """
    Function looks around in order to ckeck if there are results from previous runs of this script.

    Returns None if there is no result from previous run.
    If there are results from previous run, returns a dict of the following structure:
    {
        "tsv_respath": path_to_tsv_file_from_previous_run (str),
        "n_done_reads": number_of_successfull_requests_from_currenrt_FASTA_file (int),
    }

    :param new_dpath: path to current (corresponding to fq_fa_path file) result directory;
    :type new_dpath: str;
    :param fasta_path: path to current (corresponding to fq_fa_path file) FASTA file;
    :type fasta_path: str;
    :param blast_algorithm: BLASTn algorithm to use.
        This parameter is necessary because it is included in name of result files;
    :type blast_algorithm: str;
    """

    # "hname" means human readable name (i.e. without file path and extention)
    fasta_hname = os.path.basename(fasta_path) # get rid of absolute path
    fasta_hname = re_search(r"(.*)\.(m)?f(ast)?a", fasta_hname).group(1) # get rid of '.fasta' extention

    # Form path to result file
    tsv_res_fpath = "{}_{}_result.tsv".format(os.path.join(new_dpath, fasta_hname), blast_algorithm)

    num_done_reads = None # variable to keep number of succeffdully processed sequences

    if os.path.exists(tsv_res_fpath):

        with open(tsv_res_fpath, 'r') as res_file:
            # There can be invalid information in result file
            try:
                lines = res_file.readlines()
                num_done_reads = len(lines) - 1 # the first line is a head
                last_line = lines[-1]
                last_seq_id = last_line.split(DELIM)[0]
            except Exception as err:
                printl("\nData in result file '{}' is broken. Exact reason:".format(tsv_res_fpath))
                printl( str(err) )
                printl("Start from the beginning.")
                rename_file_verbosely(tsv_res_fpath, new_dpath)
                return None
            # end try
        # end with
    else:
        return None
    # end if

    # If we start from the beginning, we have no sequences processed
    if num_done_reads is None:
        num_done_reads = 0
    # end if

    return {
        "tsv_respath": tsv_res_fpath,
        "n_done_reads": num_done_reads,
    }
# end def look_around

# According to
# https://github.com/nanoporetech/ont_h5_validator/blob/master/h5_validator/schemas/multi_read_fast5.yaml
ont_read_signature = r"([a-f0-9]{8}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{12})"

def fmt_read_id(read_id):

    srch_ont_read = re_search(ont_read_signature, read_id)
    if srch_ont_read is None:
        return read_id.partition(' ')[0]
    else:
        return '>' + srch_ont_read.group(1)
# end def fmt_read_id


def pass_processed_seqs(fasta_file, num_done_reads, fmt_func):
    """
    Function passes sequences that have been already processed.

    :param fasta_file: FASTA file instalce;
    :type fasta_file: str;
    :param num_done_reads: amount of sequences that have been already processed;
    :type num_done_reads: int;
    :param fmt_func: function from 'FORMATTING_FUNCS' tuple;
    """

    if num_done_reads == 0:
        return None
    else:
        i = 0
        while i <= num_done_reads:
            line = fmt_func(fasta_file.readline())
            if line .startswith('>'):
                line = fmt_read_id(line)
                next_id_line = line
                i += 1
            # end if
        # end while
        return next_id_line
    # end if
# end def pass_processed_seqs


def fasta_packets(fasta, packet_size, reads_at_all, num_done_reads):
    """
    Function (actually, generator) retrieves 'packet_size' records from 'fasta'
        no matter whether is it path to FASTA file of actual FASTA data of 'str' type.
    This function will pass 'num_done_reads' sequences (i.e. they will not be processed)
        by calling 'pass_processed_files'.

    :param fasta: path to FASTA file OR actual FASTA data of 'str' type;
    :type fasta: str;
    :param packet_size: number of sequences to align in one 'blastn' launching;
    :type packet_size: int;
    :param reads_at_all: number of sequences in current file;
    :type reads_at_all: int;
    :param num_done_reads: number of sequnces in current file that have been already processed;
    :type num_doce_reads: int;
    """

    # The goal of the following if-else statement is to perform universal
    #    interface for retrieving next FASTA file no matter whether is 'fasta'
    #    path to FASTA file of actual FASTA data.

    # If 'fasta' is a path to FASTA file
    if not re_search(r"\.(m)?f(ast)?a(\.gz)?$", fasta) is None:

        how_to_open = OPEN_FUNCS[ is_gzipped(fasta) ]
        fmt_func = FORMATTING_FUNCS[ is_gzipped(fasta) ]

        # There is actually no need to close this file afterwards:
        #    CPython interpreter will close it automatically when the
        #    last reference to it will 'die'. In this case it will happen when
        #    this generator function stops iteration.
        fasta_file = how_to_open(fasta)
        # Next line etrieving will be performed as simple line-from-file reading.
        get_next_line = lambda: fmt_func(fasta_file.readline())
    
    # if 'fasta' is actual FASTA data
    else:

        fasta_lines = fasta.splitlines()
        del fasta # let interpreter get rid of this large string -- we do not need it any more
        line_i = 0 # variable for line counting

        def get_next_line():
            """
            Function retrieves element from 'fasta_lines' by 'line_i' index
                and increases 'line_i' variable by 1.
            """
            nonlocal line_i
            try:
                line = fasta_lines[line_i]
                line_i += 1
            
            except IndexError:
                # if IndexError is raised -- we have reached the end of data.
                # Simulate returning of empty string, just like io.TextIOWrapper.readline() does
                #    if end of file is reached:
                return ""
            
            else:
                return line
            # end try
        # end def get_next_line
    # end if

    # Variable that contains id of next sequence in current FASTA file.
    # If no or all sequences in current FASTA file have been already processed, this variable is None.
    # There is no way to count sequences in multi-FASTA file, accept of counting sequence IDs.
    # Therefore 'next_id_line' should be saved in memory after moment when packet is formed.
    try:
        next_id_line = pass_processed_seqs(fasta_file, num_done_reads, fmt_func)
    except UnboundLocalError:
        # This exception occurs when 'fasta_file' variable is not defined, i.e. when
        #   'fasta' is actual FASTA data, not path to file.
        # In this case we need all FASTA data.
        next_id_line = None
    # end try

    packet = ""

    line = get_next_line()
    if line.startswith('>'):
        line = fmt_read_id(line) # prune sequence ID
    # end if

    # If some sequences have been passed, this if-statement will be executed.
    # New packet should start with sequence ID line.
    if not next_id_line is None:
        packet += next_id_line+'\n'
    # end if
    packet += line+'\n' # add recently read line

    packs_at_all = reads_at_all // packet_size # Calculate total number of packets sent from current FASTA file
    if reads_at_all % packet_size > 0: # And this is ceiling (in order not to import 'math')
        packs_at_all += 1
    # end if
    packs_processed = int( num_done_reads / packet_size ) # number of successfully processed sequences
    packs_left = packs_at_all - packs_processed # number of packets left to send

    # Iterate over packets left to process
    for _ in range(packs_left):

        i = 0 # variable for counting sequenes within packet
        
        while i < packet_size:

            line = get_next_line()
            if line.startswith('>'):
                line = fmt_read_id(line) # prune sequence ID
                i += 1
            # end if
            
            if line == "": # if end of file (data) is reached
                break
            # end if
            packet += line+'\n' # add line to packet
        # end while

        if line != "":
            next_id_line = packet.splitlines()[-1] # save sequence ID next packet will start with
            packet = '\n'.join(packet.splitlines()[:-1]) # exclude 'next_id_line' from packet
        
        else:
            next_id_line = None
        # end if

        # Get list of sequence IDs:
        names = list( filter(lambda l: True if l.startswith('>') else False, packet.splitlines()) )
        names = list( map(lambda l: l.replace('>', ''), names) )

        # Just in case
        if packet == "":
            printl("Recent packet is empty")
            return
        # end if

        yield {"fasta": packet.strip(), "names": names}

        # Reset packet
        if not next_id_line is None:
            packet = next_id_line+'\n'
        else:
            return
        # end if
    # end for
# end def fasta_packets


# Variable for counting accessions of records menat to be downloaded from Genbank.
# Is used only for printing the list of accessions to console.
acc_counter = 0

# A new thread will be run.
# It will inspect size of dowbloaded file during downloading
from threading import Thread, Event

def get_gi_by_acc(acc):
    """
    Function returns GI number that corresponds to accessing passed to it.
    """

    global acc_dict
    try:
        gi_num = acc_dict[acc][0]
    except KeyError:
        printl(err_fmt("GI number error. Please, contact the developer"))
        platf_depend_exit(1)
    # end try
    
    return gi_num
# end def get_gi_by_acc


def retrieve_fastas_by_gi(gi_list, db_dir):
    """
    Function downloads set of records from Genbank according to list of GIs passed to it.
    Downloaded FASTA file will be placed in 'db_dir' directory and named 'local_seq_set.fasta'

    :param gi_list: list of GI numbers of sequences meant to be downloaded;
    :type gi_list: list<str>;
    :param db_dir: path to directory in which downloaded FASTA file will be placed;
    :type db_dir: str;

    Returns path to downloaded FASTA file of 'str'.
    """

    local_fasta = os.path.join(db_dir, "local_seq_set.fasta") # path to downloaded FASTA file
    if len(gi_list) == 0:
        return local_fasta
    # end if

    gis_del_comma = ','.join(gi_list) # GI numbers must be separated by comma in url
    # E-utilities provide us with possibility of downloading records from Genbank by GI numbers.
    retrieve_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=nuccore&id={}&rettype=fasta&retmode=text".format(gis_del_comma)
    
    global stop_wait # a flag variable that will signal waiter-function to stop executing

    def download_waiter():
        """
        Function waits untill 'local_fasta' file is downloaded.
        It prints size of downloaded data to console during downloading.
        This function just waits -- it won't bring you the menu :).
        """
        # Wait untill downloading starts
        while not os.path.exists(local_fasta):
            if stop_wait:
                return
            # end if
            sleep(1)
        # end while

        while not stop_wait:
            # Get size of downloaded data
            fsize = round(os.path.getsize(local_fasta) / (1024**2), 1) # get megabytes
            printn("\r{} - {} MB downloaded ".format(getwt(), fsize))
            sleep(1) # instant updates are not necessary
        # end while
        
        # Print total size of downloaded file (it can be deleted by this time)
        try:
            fsize = round(os.path.getsize(local_fasta) / (1024**2), 1)
        except OSError:
            pass # we can pass this ecxeption -- we do delete this file if downloading crushes
        # end try
        printl("\r{} - {} MB downloaded \n".format(getwt(), fsize))
    # end def download_waiter

    error = True
    while error:
        try:
            waiter = Thread(target=download_waiter) # create thread
            stop_wait = False # raise the flag
            waiter.start() # start waiting
            printl("\n{} - Downloading sequences for local database building started".format(getwt()))
            urllib.request.urlretrieve(retrieve_url, local_fasta) # retrieve FASTA file
        except Exception as err:
            stop_wait = True
            printl(err_fmt("error while downloading FASTA files"))
            printl( str(err) )
            printl("'barapost.py' will try again in 30 seconds")
            if os.path.exists(local_fasta):
                os.unlink(local_fasta)
            # end if
            sleep(30)
        else:
            error = False
        finally:
            stop_wait = True # lower the flag
            waiter.join() # main thread will wait until waiter function ends it's work
        # end try
    # end while

    printl("{} - Downloading is completed".format(getwt()))

    return local_fasta
# end def retrieve_fastas_by_gi


def build_local_db(acc_dict, tax_annot_res_dir):
    """
    Function builds a local indexed database with utilities from 'blast+' toolkit.

    :param acc_dict: a dictionary of accessions and record names
        Accession are keys, record names are values;
    :type acc_dict: dict<str, str>;
    :param tax_annot_res_dir: path to current result directory (each processed file has it's own result directory);
    :type tax_annot_res_dir: str;

    Returns path to builded local indexed database.
    """

    db_dir = os.path.join(tax_annot_res_dir, "local_database") # path to directory in which database will be placed
    try:
        os.makedirs(db_dir)
    except OSError as err:
        #If this directory exists

        while True:
            printl("Database directory exists in following directory:")
            printl("  '{}'".format(os.path.abspath(db_dir)))
            if len(os.listdir(db_dir)) == 0:
                # If it is empty -- nothing stops us. break and build a database
                printl("It is empty, however. Building a database...")
                break
            else:

                reply = input("""\nPress ENTER to continue aligning using this database.
Enter 'r' to remove all files in this directory and build the database from the beginning:>>""")

                if reply == "":
                    # Do not build a database, just return path to it.
                    printl() # just print blank line
                    return os.path.join(db_dir, "local_seq_set.fasta")
                
                elif reply == 'r':
                    if acc_fpath is None and len(your_own_fasta_lst) == 0:
                        printl(err_fmt("missing data to build a database from"))
                        printl(""" There is no accession file in directory '{}'
 and no FASTA file have been specified with '-l' option.""")
                        platf_depend_exit(1)
                    # end if
                    # Empty this directory and break from the loop in order to build a database.
                    for file in glob("{}{}*".format(db_dir, os.sep)):
                        os.unlink(file)
                    # end for

                    # Find directories with results left from using old database (they will be renamed):
                    putative_dirs = list( map(lambda f: os.path.join(tax_annot_res_dir, f), os.listdir(tax_annot_res_dir)) )
                    old_dirs = list( filter(lambda f: True if os.path.isdir(f) and not f.endswith("local_database") else False, putative_dirs) )
                    if len(old_dirs) > 0:
                        printl("\n Directories with results of using old database are found.")
                        printl("Renaming them...")
                        for directory in old_dirs:
                            rename_file_verbosely(directory, tax_annot_res_dir)
                        # end for
                    # end if
                    break
                else:
                    # Ask again
                    continue
                # end if
            # end if
        # end while
    # end try

    # If accession file does not exist and execution has reached here -- everything is OK --
    #    we are building local database from local files only.
    if not acc_fpath is None:
        check_connection()

        printl("""Following sequences will be downloaded from Genbank
    for further aligning on your local machine with 'blast+' toolkit:\n""")
        for i, acc in enumerate(acc_dict.keys()):
            printl(" {}. {} - '{}'".format(i+1, acc, acc_dict[acc][1]))
        # end for
        printl()
    # end if
    
    # Get list of GI numbers. Function 'get_gi_by_acc' will print the list of GIs to console.
    gi_list = list( map(get_gi_by_acc, acc_dict.keys()) )

    local_fasta = retrieve_fastas_by_gi(gi_list, db_dir) # download FASTA file

    # Add 'your own' FASTA files to database
    if not len(your_own_fasta_lst) == 0:

        # This variable counts sequences from local files.
        # It is necessary for accession deduplication.
        own_seq_counter = 0

        # Check if these files are SPAdes of a5 assembly
        spades_patt = r"NODE_[0-9]+" # this pattern will match sequence IDs generated y SPAdes
        spades_counter = 0 # variable counts number of SPAdes assembly files
        spades_assms = list() # this list will contain paths to SPAdes assembly files
        a5_patt = r"scaffold_[0-9]+" # this pattern will match sequence IDs generated y a5
        a5_counter = 0 # variable counts number of a5 assembly files
        a5_assms = list() # this list will contain paths to a5 assembly files

        for own_fasta_path in your_own_fasta_lst:

            how_to_open = OPEN_FUNCS[ is_gzipped(own_fasta_path) ]
            fmt_func = FORMATTING_FUNCS[ is_gzipped(own_fasta_path) ]

            with how_to_open(own_fasta_path) as fasta_file:
                first_seq_id = fmt_func(fasta_file.readline()) # get the first line in file (the first seq ID)
            # end with

            # if we've got SPAdes assembly
            if not re_search(spades_patt, first_seq_id) is None:
                spades_counter += 1
                spades_assms.append(own_fasta_path)
                continue
            # end if
            
            # if we've got SPAdes assembly
            if not re_search(a5_patt, first_seq_id) is None:
                a5_counter += 1
                a5_assms.append(own_fasta_path)
                continue
            # end if
        # end for

        for counter, assm_lst in zip((spades_counter, a5_counter), (spades_assms, a5_assms)):

            # If there are more than one file with assembly of one assembler,
            #    we need to distinguish these files (i.e. these assemblies).
            if counter > 1:

                # Remove this files from list -- they will be processed in a specific way
                for file in assm_lst:
                    your_own_fasta_lst.remove(file)
                # end for

                # If there are any equal basenames of files, absolute paths to these files will be used in seq IDs:
                assm_basenames = list( map(os.path.basename, assm_lst) ) # get basenames

                # If this sum is equal to length of 'assm_basenames' -- there are no duplicated basenames.
                # So, there is no need to use absolute paths.# Absolute path will be used otherwise.
                dedpul_sum = sum( map(assm_basenames.count, assm_basenames) )

                # Path conversion according to 'deduplication sum':
                if dedpul_sum == len(assm_basenames):
                    assm_lst = list( map(os.path.basename, assm_lst) )
                else:
                    assm_lst = list( map(os.path.abspath, assm_lst) )
                # end if

                # Add assembled sequences to database
                fasta_db = open(local_fasta, 'a')
                for assm_path in assm_lst:
                    printl("{} - Adding '{}' to database...".format(getwt(), os.path.basename(assm_path)))

                    how_to_open = OPEN_FUNCS[ is_gzipped(assm_path) ]
                    with how_to_open(assm_path) as fasta_file:
                        for line in fasta_file:
                            line = line.strip()
                            # You can find comments to "OWN_SEQ..." below. I don't want to duplicate them.
                            # Paths will be written to seq IDs in following way:
                            #   (_/some/happy/path.fastq_)
                            # in order to retrieve them securely with regex later.
                            if line.startswith('>'):
                                own_seq_counter += 1
                                line = ">" + "OWN_SEQ_{} (_{}_)_".format(own_seq_counter, assm_path) + line[1:]
                            # end if
                            fasta_db.write(line + '\n')
                        # end for
                    # end with
                # end for
                fasta_db.close()
            # end if
        # end for

        # No 'with open' here in order not to indent too much.
        fasta_db = open(local_fasta, 'a')
        for own_fasta_path in your_own_fasta_lst:
            printl("{} - Adding '{}' to database...".format(getwt(), os.path.basename(own_fasta_path)))

            how_to_open = OPEN_FUNCS[ is_gzipped(own_fasta_path) ]
            with how_to_open(own_fasta_path) as fasta_file:
                for line in fasta_file:
                    line = fmt_func(line)
                    # 'makeblastdb' considers first word (sep. is space) as sequence ID
                    #   and throws an error if there are duplicate IDs.
                    # In order not to allow this duplication we'll create our own sequence IDs:
                    #   'OWN_SEQ_<NUMBER>' and write it in the beginning of FASTA record name.
                    if line.startswith('>'):
                        own_seq_counter += 1
                        line = ">" + "OWN_SEQ_{} ".format(own_seq_counter) + line[1:]
                    # end if
                    
                    fasta_db.write(line + '\n')
                # end for
            # end with
        # end for
        fasta_db.close()
    # end if

    # Configure command line
    make_db_cmd = "makeblastdb -in {} -parse_seqids -dbtype nucl".format(local_fasta)
    exit_code = os.system(make_db_cmd) # make a blast-format database
    if exit_code != 0:
        printl(err_fmt("error while making the database"))
        platf_depend_exit(exit_code)
    # end if
    
    printl("""{} - Database is successfully created:
  '{}'\n""".format(getwt(), local_fasta))

    printl("{} - Database index creating started".format(getwt()))
    # Configure command line
    make_index_cmd = "makembindex -input {} -iformat blastdb -verbosity verbose".format(local_fasta)
    exit_code = os.system(make_index_cmd) # create an index for the database
    if exit_code != 0:
        printl(err_fmt("error while creating database index"))
        platf_depend_exit(exit_code)
    # end if
    
    printl("{} - Database index has been successfully created\n".format(getwt()))

    # Gzip downloaded FASTA file in order to save space on disk
    printl("Gzipping FASTA file:\n '{}'\n".format(local_fasta))

    # GNU gzip utility is faster, but there can be presence of absence of it
    gzip_util = "gzip"
    util_found = False
    for directory in os.environ["PATH"].split(os.pathsep):
        if os.path.isdir(directory) and gzip_util in os.listdir(directory):
            util_found = True
            break
        # end if
    # end for

    if util_found:
        os.system("{} {}".format(gzip_util, local_fasta))
    else:
        from shutil import copyfileobj as shutil_copyfileobj
        # form .fasta.gz file 'by hand'
        with open(local_fasta, 'rb') as fasta_file, open_as_gzip(local_fasta+".gz", "wb") as fagz_file:
            shutil_copyfileobj(fasta_file, fagz_file)
        # end with
        os.unlink(local_fasta) # remove source FASTA file, not the database
    # end if

    return local_fasta
# end def build_local_db


def launch_blastn(packet, blast_algorithm):
    """
    Function launches 'blastn' utility from "BLAST+" toolkit and returns it's response.

    :param pacekt: FASTA data meant to be processend by 'blastn';
    :type packet: str;
    :param blast_algorithm: blastn algorithm to use;
    :type blast_algorithm: str;
    """

    # Indexed discontiguous searches are not supported:
    #    https://www.ncbi.nlm.nih.gov/books/NBK279668/#usermanual.Megablast_indexed_searches

    # Algorithms in 'blast+' are named in a little different way comparing to BLAST server.
    # In order to provide full cli-interface compatibility with 'prober.py' I will merely change values here.
    # I change these values in this function and not globally in order not to influence names of result files.
    if blast_algorithm == "megaBlast":
        blast_algorithm = "megablast"
    elif blast_algorithm == "discoMegablast":
        blast_algorithm = "dc-megablast"
    # end if

    if blast_algorithm != "dc-megablast":
        use_index = "true"
    else:
        use_index = "false"
    # end if

    # PID of current process won't change, so we can use it to mark query files.
    # 'paket's are too large to pass them to 'subprocess.Popen' as stdin,
    #    therefore we need to use these query files.
    query_path = os.path.join(queries_tmp_dir, "query{}_tmp.fasta".format(os.getpid()))

    with open(query_path, 'w') as query_file:
        query_file.write(packet)
    # end with

    # Configure command line
    blast_cmd = "blastn -query {} -db {} -outfmt 5 -task {} -max_target_seqs 1 -use_index {}".format(query_path,
        local_fasta, blast_algorithm, use_index)

    pipe = sp_Popen(blast_cmd, shell=True, stdout=sp_PIPE, stderr=sp_PIPE)
    stdout_stderr = pipe.communicate()

    if pipe.returncode != 0:
        printl(err_fmt("error while aligning a sequence against local database"))
        printl(stdout_stderr[1].decode("utf-8"))
        exit(pipe.returncode)
    # end if

    return stdout_stderr[0].decode("utf-8")
# end def launch_blastn


def remove_tmp_files(*paths):
    """
    Function removes files passed to it.
    Actually, passed arguments are paths ('str') to files meant to be removed.
    """
    for path in paths:
        if os.path.exists(path):
            os.unlink(path)
        # end if
    # end for
# end def remove_tmp_files


get_phred33 = lambda q_symb: ord(q_symb) - 33

def get_read_avg_qual(qual_str):
    """
    Function calculates average Phred33 quality of a quality string passed to it.

    :param qual_str: FASTQ quality string;
    :type qual_str: str;
    """

    phred33 = map(get_phred33, list(qual_str))
    read_qual = round( sum(phred33) / len(qual_str), 2 )
    return read_qual
# end def get_read_avg_qual


def configure_qual_dict(fastq_path):
    """
    Fucntion coufigures quality dictionary:
        keys are sequence IDs, values are their average Phred33 qualities.

    :param fastq_path: path to FASTQ file meant to be processed;
    :type fastq_path: str;
    """

    qual_dict = dict()
    how_to_open = OPEN_FUNCS[ is_gzipped(fastq_path) ]
    fmt_func = FORMATTING_FUNCS[ is_gzipped(fastq_path) ]

    with how_to_open(fastq_path) as fastq_file:
        counter = 1 # variable for counting lines
        line = fmt_func(fastq_file.readline())
        while line != "":
            if counter == 1:
                seq_id = intern( fmt_read_id(line).replace('>', '') )
            # end if
            
            counter += 1
            line = fmt_func(fastq_file.readline())
            if counter == 4:
                qual_dict[seq_id] = get_read_avg_qual(line)
                counter = 0
            # end if
        # end while
    # end with

    return qual_dict
# end def configure_qual_dict


def parse_align_results_xml(xml_text, seq_names, qual_dict):
    """
    Function parses BLAST xml response and returns tsv lines containing gathered information:
        1. Query name.
        2. Hit name formatted by 'format_taxonomy_name()' function.
        3. Hit accession.
        4. Length of alignment.
        5. Percent of identity.
        6. Percent of gaps.
        7. E-value.
    Erroneous tsv lines that function may produce:
        1. "<query_name>\\tQuery has been lost: ERROR, Bad Gateway"
            if data packet has been lost.
            # end if
        2. "<query_name>\\tQuery has been lost: BLAST ERROR"
            if BLAST error occured.
            # end if
        3. "<query_name>\\tNo significant similarity found"
            if no significant similarity has been found
            # end if
        Type of return object: list<str>.
    """

    result_tsv_lines = list()

    # /=== Validation ===/

    if "Bad Gateway" in xml_text:
        printl('\n' + '=' * 45)
        printl(getwt() + " - ERROR! Bad Gateway! Data from last packet has lost.")
        printl("It would be better if you restart the script.")
        printl("Here are names of lost queries:")
        for i, name in enumerate(seq_names):
            printl("{}. '{}'".format(i+1, name))
            result_tsv_lines.append(name + DELIM + "Query has been lost: ERROR, Bad Gateway")
        # end for
        
        input("Press ENTER to continue...")

        return result_tsv_lines
    # end if

    if "to start it again" in xml_text:
        printl('\n' + getwt() + "BLAST ERROR!")

        printl("Here are names of lost queries:")
        for i, name in enumerate(seq_names):
            printl("{}. '{}'".format(i+1, name))
            result_tsv_lines.append(name + DELIM +"Query has been lost: BLAST ERROR")
        # end for

        input("Press ENTER to continue...")
        return result_tsv_lines
    # end if

    # /=== Parse BLAST XML response ===/

    root = ElementTree.fromstring(xml_text) # get tree instance

    # Iterate over "Iteration" and "Iteration_hits" nodes
    for iter_elem, iter_hit in zip(root.iter("Iteration"), root.iter("Iteration_hits")):
    
        # "Iteration" node contains query name information
        query_name = iter_elem.find("Iteration_query-def").text

        query_len = iter_elem.find("Iteration_query-len").text

        if not qual_dict is None:
            ph33_qual = qual_dict[query_name]
            miscall_prop = round(10**(ph33_qual/-10), 3)
            accuracy = round( 100*(1 - miscall_prop), 2 ) # expected percent of correctly called bases
        else:
            # If FASTA file is processing, print dashed in quality columns
            ph33_qual = "-"
            accuracy = "-" # expected percent of correctly called bases
        # end if

        # If there are any hits, node "Iteration_hits" contains at least one "Hit" child
        hit = iter_hit.find("Hit")
        if hit is not None:

            # Get full hit name (e.g. "Erwinia amylovora strain S59/5, complete genome")
            hit_name = hit.find("Hit_def").text
            # Format hit name (get rid of stuff after comma)
            hit_taxa_name = hit_name[: hit_name.find(',')] if ',' in hit_name else hit_name
            hit_taxa_name = hit_taxa_name.replace(" complete genome", "") # sometimes there are no comma before it
            hit_taxa_name = hit_taxa_name.replace(' ', '_')

            hit_acc = hit.find("Hit_accession").text # get hit accession

            # Find the first HSP (we need only the first one)
            hsp = next(hit.find("Hit_hsps").iter("Hsp"))

            align_len = hsp.find("Hsp_align-len").text.strip()

            pident = hsp.find("Hsp_identity").text # get number of matched nucleotides

            gaps = hsp.find("Hsp_gaps").text # get number of gaps

            evalue = hsp.find("Hsp_evalue").text # get e-value
            # If E-value is low enough -- add this subject sequence to 'acc_dict' to further downloading

            pident_ratio = round( float(pident) / int(align_len) * 100, 2)
            gaps_ratio = round( float(gaps) / int(align_len) * 100, 2)

            # Append new tsv line containing recently collected information
            result_tsv_lines.append( DELIM.join( (query_name, hit_taxa_name, hit_acc, query_len,
                align_len, pident, gaps, evalue, str(ph33_qual), str(accuracy)) ))
        else:
            # If there is no hit for current sequence
            result_tsv_lines.append(DELIM.join( (query_name, "No significant similarity found", "-", query_len,
                "-", "-", "-", "-", str(ph33_qual), str(accuracy)) ))
        # end if
    # end for
    return result_tsv_lines
# end def parse_align_results_xml


def write_result(res_tsv_lines, tsv_res_path):
    """
    Function writes result of blasting to result tsv file.

    :param res_tsv_lines: tsv lines returned by 'parse_align_results_xml()' funciton;
    :type res_tsv_lines: list<str>;
    :param tsv_res_path: path to reslut tsv file;
    :type tsv_res_path: str;
    """

    # If there is no result tsv fil -- create it and write a head of the table.
    if not os.path.exists(tsv_res_path):
        with open(tsv_res_path, 'w') as tsv_res_file:
            tsv_res_file.write(DELIM.join( ["QUERY_ID", "HIT_NAME", "HIT_ACCESSION", "QUERY_LENGTH",
                "ALIGNMENET_LENGTH", "IDENTITY", "GAPS", "E-VALUE", "AVG_PHRED33", "ACCURACY(%)"] ) + '\n')
        # end with
    # end if
    # Write reslut tsv lines to this file
    with open(tsv_res_path, 'a') as tsv_res_file:
        for line in res_tsv_lines:
            tsv_res_file.write(line + '\n')
        # end for
    # end with
# end def write_result


def get_curr_res_dpath(fq_fa_path, tax_annot_res_dir):
    """
    Function configures and returns the path to result directory for particular FASTA or FASTQ file.
    Result directory is nested in 'tax_annot_res_dir' and is named according to name of FASTA/FASTQ file.
    E.g. if file 'some_reads.fastq' is processing, it's result directory will be named 'some_reads'.

    :param fq_fa_path: path to FASTA/FASTQ file that is procesing;
    :type fq_fa_path: str;
    :param tax_annot_res_dir: path to directory with results of 'prober.py';
    :type tax_annot_res_dir: str;

    Returns path to result directory that was recenly created of 'str' type.
    """

    # dpath means "directory path"
    new_dpath = os.path.join(tax_annot_res_dir, os.path.basename(fq_fa_path)) # get rid of absolute path
    new_dpath = re_search(r"(.*)\.(m)?f(ast)?(a|q)", new_dpath).group(1) # get rid of extention

    return new_dpath
# end def get_curr_res_dir


def create_curr_res_dir(new_dpath):
    """
    Function creates a result directory for particular FASTA or FASTQ file.
    Result directory is nested in 'tax_annot_res_dir' and is named according to name of FASTA/FASTQ file.
    E.g. if file 'some_reads.fastq' is processing, it's result directory will be named 'some_reads'.

    :param fq_fa_path: path to FASTA/FASTQ file that is procesing;
    :type fq_fa_path: str;
    :param tax_annot_res_dir: path to directory with results of 'prober.py';
    :type tax_annot_res_dir: str;
    """

    # Create this directory, if it does not exist
    if not os.path.exists(new_dpath):
        try:
            os.makedirs(new_dpath)
        except OSError as err:
            printl(err_fmt("error while creating directory '{}'".format(new_dpath)))
            printl( str(err) )
            platf_depend_exit(1)
        # end try
    # end if
# end def create_curr_res_dir


def configure_acc_dict(acc_fpath):
    """
    Fucntion couffigures accession dictionary according to accessin file generated by 'prober.py':
       keys are accessions, values are tuples of the following format:
        (<GI_number>, <sequence_name_aka_definition>).

    :param acc_fpath: path to accession file generated by 'prober.py';
    :type acc_fpath: str;

    Returns accession dictionary described above.
    """

    acc_dict = dict()
    global your_own_fasta_lst

    # if database will be builded only from 'your own' FASTA files -- return empty dict
    if acc_fpath is None:
        return acc_dict
    # end if

    with open(acc_fpath, 'r') as acc_file:
        lines = acc_file.readlines()

        for line in lines:
            line = line.strip()
            # Ignore ampty lines, commented lines and head of the table:
            if line != "" and not line.startswith('#') and not line.startswith("ACCESSION"):

                # Handle situation if user writes path to his own FASTA file
                #  (that is meant to be added to DB) to accession file.
                if not os.path.exists(line) and not re_search(r".*\.(m?)f(ast)?a(\.gz?)", line):
                    try:
                        line_splt = line.split(DELIM)
                        acc = intern(line_splt[0])
                        gi = line_splt[1]
                        name = line_splt[2]
                        acc_dict[acc] = (gi, name)
                    
                    except IndexError as inderr:
                        printl(err_fmt("invalid data in file '{}'!".format(acc_fpath)))
                        printl("Seems, you have written path to file that does not exist or not a FASTA file.")
                        printl("Here is this invalid line:\n   '{}'".format(line))
                        platf_depend_exit(1)
                    # end try
                else:
                    your_own_fasta_lst.append(line)
                # end if
            # end if
        # end for
    # end with

    if len(acc_dict) == 0:
        printl(err_fmt("no accession information found in file '{}".format(acc_fpath)))
        platf_depend_exit(1)
    # end if

    return acc_dict
# end def configure_acc_dict


# |===== Functionality for parallel computation =====|

# Comments to following functions:
# 1. process_multiple_files;
# 2. process_single_file_in_paral

# 1. 'curr_fasta' is a dict of the following structure:
#    {
#        "fpath": path_to_FASTA_file (str),
#        "nreads": number_of_reads_in_this_FASTA_file (int)
#    }
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 2. 'previous_data' is a dict of the following structure:
#    {
#        "tsv_respath": path_to_tsv_file_from_previous_run (str),
#        "n_done_reads": number_of_successfull_requests_from_currenrt_FASTA_file (int),
#    }
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 3. 'packet' is a dict of the following structure:
#    {
#        "fasta": FASTA_data_containing_query_sequences (str),
#        "names": list_of_sequence_ids_from_FASTA_file (list<str>)
#    }


def spread_files_equally(fq_fa_list, n_thr):
    """
    Function distributes files among processes equally.

    :param fq_fa_list: list of paths to files meant to be processed:
    :type fq_fa_list: list<str>;
    :param n_thr: number of therads to launch;
    :type n_thr: int;
    """

    sublist_size = len(fq_fa_list) // n_thr

    # Processes [0, (n_thr-1)] will obtain equally 'sublist_size' files:
    start_pos = 0
    for i in range(n_thr - 1):
        yield fq_fa_list[start_pos : start_pos+sublist_size]
        start_pos += sublist_size
    # end for

    # Give the rest of data to the last unlucky process:
    yield fq_fa_list[start_pos :]

# end def spread_files_equally


def init_proc_many_files(print_lock_buff, inc_lock_buff,
        inc_val_buff):
    """
    Function that initializes global variables that all processes shoud have access to.
    This function is meant to be passed as 'initializer' argument to 'multiprocessing.Pool' function.
    Function works when 'many_files'-parallel mode is running.

    :param print_lock_buff: lock that synchronizes printing to the console;
    :type print_lock_buff: multiprocessing.Lock;
    :param perc_inc_lock_buff: lock that synchronizes updating 'perc_array';
    :type perc_inc_lock_buff: multiprocessing.Lock;
    """

    global print_lock
    print_lock = print_lock_buff

    global inc_lock
    inc_lock = inc_lock_buff

    global inc_val
    inc_val = inc_val_buff

# end def init_proc_many_files


def process_multiple_files(fq_fa_list, parallel=False):
    """
    Function performs 'many_files'-parallel mode of single-thread mode.
    They differ only in ptinting to the console.

    :param fq_fa_list: list of paths to FASTA and FASTQ files meant to be processed;
    :type fq_fa_list: list<str>;
    :param parallel: flag indicating if parallel mode if enabled.
        Influences only on printing to the console;
    :type parallel: bool;
    """

    if not parallel:
        global inc_val
    # end if

    # Iterate over source FASTQ and FASTA files
    for i, fq_fa_path in enumerate(fq_fa_list):

        # Configure quality dictionary
        qual_dict = configure_qual_dict(fq_fa_path) if is_fastq(fq_fa_path) else None

        # Create the result directory with the name of FASTQ of FASTA file being processed:
        new_dpath = get_curr_res_dpath(fq_fa_path, tax_annot_res_dir)

        create_curr_res_dir(new_dpath) # create this directory, if it doesn't exist

        # Convert FASTQ file to FASTA (if it is FASTQ) and get it's path and number of sequences in it:
        curr_fasta = fastq2fasta(fq_fa_path, new_dpath)

        # "hname" means human readable name (i.e. without file path and extention)
        fasta_hname = os.path.basename(curr_fasta["fpath"]) # get rid of absolure path
        fasta_hname = re_search(r"(.*)\.(m)?f(ast)?a", fasta_hname).group(1) # get rid of file extention

        # Look around and ckeck if there are results of previous runs of this script
        # If 'look_around' is None -- there is no data from previous run
        previous_data = look_around(new_dpath, curr_fasta["fpath"],
            blast_algorithm)

        if previous_data is None: # If there is no data from previous run
            num_done_reads = 0 # number of successfully processed sequences
            tsv_res_path = "{}_{}_result.tsv".format(os.path.join(new_dpath,
                fasta_hname), blast_algorithm) # form result tsv file path
        else: # if there is data from previous run
            num_done_reads = previous_data["n_done_reads"] # get number of successfully processed sequences
            tsv_res_path = previous_data["tsv_respath"] # result tsv file sholud be the same as during previous run
        # end if

        if num_done_reads == curr_fasta["nreads"]:
            if parallel:
                print_lock.acquire()
            # end if
            printl("\nFile '{}' has been already completely processed.".format(fq_fa_path))
            printl("Omitting it.")
            if parallel:
                print_lock.release()
            # end if
            if not parallel:
                inc_val += 1
            else:
                with inc_lock:
                    inc_val.value += 1
                # end with
            # end if
            continue
        # end if

        packs_at_all = (curr_fasta["nreads"] - num_done_reads) // packet_size # Calculate total number of packets sent from current FASTA file
        if (curr_fasta["nreads"] - num_done_reads) % packet_size != 0: # And this is ceiling (in order not to import 'math')
            packs_at_all += 1
        # end if
        packs_processed = int( num_done_reads / packet_size ) # number of successfully processed sequences
        packs_left = packs_at_all - packs_processed

        for pack_i, packet in enumerate(fasta_packets(curr_fasta["fpath"],
            packet_size, curr_fasta["nreads"], num_done_reads)):

            # Align the packet
            align_xml_text = launch_blastn(packet["fasta"], blast_algorithm)

            # Get result tsv lines
            result_tsv_lines = parse_align_results_xml(align_xml_text,
                packet["names"], qual_dict)

            # Write the result to tsv
            write_result(result_tsv_lines, tsv_res_path)
        # end for
        
        if not parallel:
            inc_val += 1
        else:
            with inc_lock:
                inc_val.value += 1
            # end with
        # end if
    # end for
    remove_tmp_files( os.path.join(queries_tmp_dir, "query{}_tmp.fasta".format(os.getpid())) )
# end def process_multiple_files


def init_proc_single_file_in_paral(print_lock_buff, write_lock_buff, pack_i_buff, pack_i_lock_buff):
    """
    Function that initializes global variables that all processes shoud have access to.
    This function is meant to be passed as 'initializer' argument to 'multiprocessing.Pool' function.
    Function works when 'few_files'-parallel mode is running.

    :param print_lock_buff: lock that synchronizes printing to the console;
    :type print_lock_buff: multiprocessing.Lock;
    :param write_lock_buff: lock that synchronizes wriiting to result file;
    :type write_lock_buff: multiprocessing.Lock;
    :param pack_i_buff: integer number representing number of processed packets;
    :type pack_i_buff: multiprocessing.Value;
    :param pack_i_lock_buff: lock that synchronizes incrementing 'pack_i' variable;
    :type pack_i_lock_buff: multiprocessing.Lock;
    """
    global print_lock
    print_lock = print_lock_buff

    global write_lock
    write_lock = write_lock_buff

    global pack_i
    pack_i = pack_i_buff

    global pack_i_lock
    pack_i_lock = pack_i_lock_buff
# end def init_proc_single_file_in_paral


def process_part_of_file(data, tsv_res_path, qual_dict, seqs_left):
    """
    Function preforms processing part of file in 'few_files'-parallel mode.

    :param data: FASTA data meant to be processed;
    :type data: str;
    :param tsv_res_path: path to result TSV file;
    :type tsv_res_path: str;
    :param qual_dict: quality dictionary;
    :type qual_dict: dcit<str: float>;
    :param seqs_left: numer of sequences to process;
    :type seqs_left: int;
    """

    # This character cannot appear anywhere accept the begining of sequence ID in FASTA file,
    #    therefor it is a valid check:
    seqs_at_all = data.count('>')

    for packet in fasta_packets(fasta=data, packet_size=packet_size,
            reads_at_all=seqs_at_all, num_done_reads=0):

        # Align the packet
        align_xml_text = launch_blastn(packet["fasta"], blast_algorithm)

        # Get result tsv lines
        result_tsv_lines = parse_align_results_xml(align_xml_text,
            packet["names"], qual_dict)

        # Write the result to tsv
        with write_lock:
            write_result(result_tsv_lines, tsv_res_path)
        # end with

        with pack_i_lock:
            pack_i.value += len(result_tsv_lines)
        # end with

    # end for
    remove_tmp_files( os.path.join(queries_tmp_dir, "query{}_tmp.fasta".format(os.getpid())) )
# end def process_part_of_file


def process_single_file_in_paral(fq_fa_path, i):
    """
    Function preforms "few_files"-parallel mode.

    :param fq_fa_path: path to FASTA or FASTQ file meant to be processed;
    :type fq_fa_path: str;
    :param i: number of this file;
    :type i: int;
    """
    global inc_val

    # Configure quality dictionary
    qual_dict = configure_qual_dict(fq_fa_path) if is_fastq(fq_fa_path) else None

    # Create the result directory with the name of FASTQ of FASTA file being processed:
    new_dpath = get_curr_res_dpath(fq_fa_path, tax_annot_res_dir)
    create_curr_res_dir(new_dpath) # create this directory, if it doesn't exist

    # Convert FASTQ file to FASTA (if it is FASTQ) and get it's path and number of sequences in it:
    curr_fasta = fastq2fasta(fq_fa_path, new_dpath)

    # "hname" means human readable name (i.e. without file path and extention)
    fasta_hname = os.path.basename(curr_fasta["fpath"]) # get rid of absolure path
    fasta_hname = re_search(r"(.*)\.(m)?f(ast)?a", fasta_hname).group(1) # get rid of file extention

    # Look around and ckeck if there are results of previous runs of this script
    # If 'look_around' is None -- there is no data from previous run
    previous_data = look_around(new_dpath, curr_fasta["fpath"],
        blast_algorithm)

    if previous_data is None: # If there is no data from previous run
        num_done_reads = 0 # number of successfully processed sequences
        tsv_res_path = "{}_{}_result.tsv".format(os.path.join(new_dpath,
            fasta_hname), blast_algorithm) # form result tsv file path
    else: # if there is data from previous run
        num_done_reads = previous_data["n_done_reads"] # get number of successfully processed sequences
        tsv_res_path = previous_data["tsv_respath"] # result tsv file sholud be the same as during previous run
    # end if

    if num_done_reads == curr_fasta["nreads"]:
        printl("\nFile '{}' have been already completely processed.".format(fq_fa_path))
        printl("Omitting it.")
        return
    # end if

    how_to_open = OPEN_FUNCS[ is_gzipped(fq_fa_path) ]
    fmt_func = FORMATTING_FUNCS[ is_gzipped(fq_fa_path) ]

    print_lock = mp.Lock() # lock for printing to the console
    write_lock = mp.Lock() # lock for writing to the result file
    pack_i = mp.Value('i', 0) # variablefor counting packets
    pack_i_lock = mp.Lock() # lock for incrementing 'pack_i'

    file_part_size = curr_fasta["nreads"] // n_thr
    if curr_fasta["nreads"] // n_thr != 0:
        file_part_size += 1
    # end if

    # If n_thr > len(fq_fa_list) it is better to count sequences.
    seqs_left = curr_fasta["nreads"] - num_done_reads

    pool = mp.Pool(n_thr, initializer=init_proc_single_file_in_paral,
        initargs=(print_lock, write_lock, pack_i, pack_i_lock))

    pool.starmap(process_part_of_file, [(file_part["fasta"], tsv_res_path, qual_dict, seqs_left) for file_part in fasta_packets(curr_fasta["fpath"],
        file_part_size, curr_fasta["nreads"], num_done_reads)])
    # Reaping zombies
    pool.close()
    pool.join()
    inc_val += 1
# end def process_single_file_in_paral



# =/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=
#                       |===== Proceed =====|

printl(" - Output directory: '{}';".format(tax_annot_res_dir))
printl(" - Packet size: {} sequences;".format(packet_size))
printl(" - BLAST algorithm: {};".format(blast_algorithm))
printl(" - Threads: {};\n".format(n_thr))

s_letter = '' if len(fq_fa_list) == 1 else 's'
printl(" {} file{} will be processed.".format( len(fq_fa_list), s_letter))
logfile.write("Here they are:\n")
for i, path in enumerate(fq_fa_list):
    logfile.write("    {}. '{}'\n".format(i+1, path))
# end for

if not len(your_own_fasta_lst) == 0:
    preposition = " besides downloaded ones" if not acc_fpath is None else ""
    printl("\n Following FASTA files will be added to database{}:".format(preposition))
    for i, path in enumerate(your_own_fasta_lst):
        printl("   {}. '{}'".format(i+1, path))
    # end for
# end if

printl('-'*30 + '\n')

# It is a dictionary of accessions and record names.
# Accessions are keys, tuples of GI numbers record names are values.
# This dictionary is filled while processing and at the beginning of continuation.
acc_dict = configure_acc_dict(acc_fpath)

# Build a database
local_fasta = build_local_db(acc_dict, tax_annot_res_dir)

# Create temporary directory for query files:
queries_tmp_dir = os.path.join(tax_annot_res_dir, "queries-tmp")
if not os.path.isdir(queries_tmp_dir):
    try:
        os.makedirs(queries_tmp_dir)
    except OSError as oserr:
        printl(err_fmt("unable to create query directory"))
        printl( str(oserr) )
        printl("Barapost just tried to create directory '{}' and crushed.".format(queries_tmp_dir))
        platf_depend_exit(1)
    # end try
# end if

# Value that contains number of processed files:
global inc_val


def status_printer(get_inc_val, stop):
    """
    Function meant to be launched as threading.Thread in order to indicate progress each second.

    :param get_inc_val: function that returns 'inc_val' value -- the number of processed files;
    :param stop: event that will signal printer when to stop;
    :type stop: threading.Event;
    """

    nfiles = len(fq_fa_list)
    printn("{} - 0/{} files processed. Working...".format(getwt(), nfiles))
    saved_val = get_inc_val()
    while stop.is_set():
        loc_inc_val = get_inc_val()
        printn("\r{} - {}/{} files processed. Working...".format(getwt(), loc_inc_val, nfiles))
        if loc_inc_val != saved_val:
            logfile.write("{} - {}/{} files processed.\n".format(getwt(), loc_inc_val, nfiles))
            saved_val = get_inc_val()
        sleep(1)
    # end while
    printn("\r{} - {}/{} files processed.".format(getwt(), get_inc_val(), nfiles) +
        ' '*len(" Working..."))
    logfile.write("{} - {}/{} files processed.\n".format(getwt(), get_inc_val(), nfiles))
# end def status_printer

# Proceeding.
# The main goal is to isolate processes from one another.
# 
# Two situations are available:
#   1. number of threads <= number of files meant to be processed ('many_files'-parallel mode):
#      Files will be distribured equally among processes.
#      Processes interact with one another only while printing something to the console
#      for user's entertainment.
#   2. number of threads > number of files meant to be processed ('few_files'-parallel mode):
#      Files will be processed one by one. They will be divided into equal blocks,
#      and these blocks will be distributed among processes.
#      Processes interact with one another while writing to result file and
#      while printing something to the console for user's entertainment.

stop = Event()
stop.set() # raise the flag

if n_thr <= len(fq_fa_list):
    if n_thr != 1:

        print_lock = mp.Lock() # lock for printing
        inc_lock = mp.Lock() # lock for updating 'perc_array'
        inc_val = mp.Value('i', 0)
        get_inc_val = lambda: inc_val.value # access shared 'inc_val' value

        # Launch printer
        printer = Thread(target=status_printer, args=(get_inc_val, stop), daemon=True) # create thread
        printer.start() # start waiting

        pool = mp.Pool(n_thr, initializer=init_proc_many_files, initargs=(print_lock, inc_lock, inc_val))
        pool.starmap(process_multiple_files, [ (fq_fa_sublist, True) for fq_fa_sublist in spread_files_equally(fq_fa_list, n_thr) ])

        # Reaping zombies
        pool.close()
        pool.join()

    else:
        inc_val = 0
        get_inc_val = lambda: inc_val # merely return this value (1 thread)

        # Launch printer
        printer = Thread(target=status_printer, args=(get_inc_val, stop), daemon=True) # create thread
        printer.start() # start waiting

        # Single-thread mode do not differ much from 'many_files'-parallel mode.
        process_multiple_files(fq_fa_list, parallel=False)
    # end if
else:

    inc_val = 0
    # Merely return this value ('inc_val' is incrementing in the main process)
    get_inc_val = lambda: inc_val

    # Launch printer
    printer = Thread(target=status_printer, args=(get_inc_val, stop), daemon=True) # create thread
    printer.start() # start waiting

    for i, fq_fa_path in enumerate(fq_fa_list):
        process_single_file_in_paral(fq_fa_path, i)
    # end for

# end if

# Stop printer
stop.clear() # lower the flag
printer.join()
printl()

# Remove all in 'queries_tmp_dir'
try:
    for qpath in glob( os.path.join(queries_tmp_dir, '*') ):
        os.unlink(qpath)
    # end for
    os.rmdir(queries_tmp_dir)
except OSError as oserr:
    printl(err_fmt("unable to delete directory '{}'".format(queries_tmp_dir)))
    printl( str(oserr) )
    printl("Don't worry -- barapost has completed it's job just fine,")
    printl("   the only thing that some temporary files are left in the directory mentioned above.\n")
# end try

end_time = time()
printl( '\n'+getwt() + " ({}) ".format(strftime("%Y-%m-%d %H:%M:%S", localtime(end_time))) + "- Task is completed!\n")
logfile.close()
platf_depend_exit(0)