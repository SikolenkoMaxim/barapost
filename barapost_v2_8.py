#!/usr/bin/env python3
# -*- coding: utf-8 -*-

# Version 2.8
# 18.09.2019 edition

# |===== Check python interpreter version =====|

from sys import version_info as verinf

if verinf.major < 3:#{
    print( "\nYour python interpreter version is " + "%d.%d" % (verinf.major, verinf.minor) )
    print("\tPlease, use Python 3!\a")
    # In python 2 'raw_input' does the same thing as 'input' in python 3.
    # Neither does 'input' in python2.
    raw_input("Press ENTER to exit:")
    exit(1)
#}

print("\n |=== barapost.py (version 2.8) ===|\n")

# |===== Stuff for dealing with time =====|

from time import time, strftime, gmtime, sleep, localtime
start_time = time()


def get_work_time():#{
    return strftime("%H:%M:%S", gmtime(time() - start_time))
#}

# |===========================================|

import os
from re import search as re_search
from gzip import open as open_as_gzip # input files might be gzipped
from xml.etree import ElementTree # for retrieving information from XML BLAST report
from sys import intern
from subprocess import Popen as sp_Popen, PIPE as sp_PIPE
import multiprocessing as mp

import http.client
import urllib.request
from urllib.error import HTTPError
import urllib.parse
import socket


# |===== Function that asks to press ENTER on Windows =====|

from sys import platform

def platf_depend_exit(exit_code):#{
    """
    Function asks to press ENTER press on Windows
        and exits after that.

    :type exit_code: int;
    """
    if platform.startswith("win"):
        input("Press ENTER to exit:")
    exit(exit_code)
#}


def print_error(text):#{
    "Function for printing error messages"
    print("\n   \a!! - ERROR: " + text + '\n')
#}


from sys import stdout as sys_stdout
def printn(text):#{
    """
    Function prints text to the console without adding '\n' in the end of the line.
    Why not just to use 'print(text, end="")'?
    In order to display informative error message if Python 2.X is launched
        instead if awful error traceback.
    """
    sys_stdout.write(text)
#}


# |===== Handle command line arguments =====|
help_msg = """
DESCRIPTION:\n
 barapost.py -- script is designed for determinating the taxonomic position
    of nucleotide sequences by aligning each of them with 'blastn' from 'blast+' toolkit
    and regarding the best hit.\n
 'barapost.py' is meant to be used just after 'prober.py'.
 Script processes FASTQ and FASTA (as well as '.fastq.gz' and '.fasta.gz') files.\n
 Results of the work of this script are written to TSV file,
    that can be found in result directory.\n
 FASTQ files processed by this script are meant to be sorted afterwards by 'fastQA_sorted.py'.
----------------------------------------------------------

Default parameters:\n
- all FASTQ and FASTA files in current directory will be processed;
- packet size (see '-p' option): 100 sequences;
- algorithm (see '-a' option): 'megaBlast';

  Default behavior is to download records-hits from Genbank according to results
of work of 'prober.py' script, build an indexed local database which consists of
downloaded sequences, and continue aligning with 'blast+' toolkit in order to save time.
----------------------------------------------------------

OPTIONS:\n
    -h (--help) --- show help message;\n
    -f (--infile) --- input FASTQ or FASTA (or '.fastq.gz', '.fasta.gz') file;
        You can specify multiple input files with this option (see EXAMPLES #2);\n
    -d (--indir) --- directory which contains FASTQ of FASTA files meant to be processed.
        I.e. all FASTQ and FASTA files in this direcory will be processed;
        Input files can be gzipped.\n
    -p (--packet-size) --- size of the packet, i.e. number of sequence to blast in one request.
        Value: integer number [1, 500]. Default value is 100;\n
    -a (--algorithm) --- BLASTn algorithm to use for aligning.
        Available values: 'megaBlast', 'discoMegablast', 'blastn'.
        Default is megaBlast;\n
    -r (--prober-result-dir) --- result directory genearted by script 'prober.py'
        This is directory specified to 'prober.py' by '-o' option.
        Default value is "prober_result", since it is the default name of
        output directory generated by "prober.py".\n
    -l (--local-fasta-to-db) --- your own FASTA file that will be added to downloaded database
        or used instead of it if you omit 'prober.py' step;\n
    -o (--outdir) --- output directory. Can be used only if '-l' option is specified.
        The reason is that results of 'barapost.py' should be written to the directory that contains
        accession file generated by "prober.py". If you omit "prober.py" stage and specify your own FASTA
        files that are meant to be used as database, you may specify output directory with this option.
----------------------------------------------------------

EXAMPLES:\n
  1) Process one FASTQ file with default settings.
     File 'reads.fastq' has been already processed by 'prober.py'.
     Results of prober.py work are in directory 'prober_outdir':\n
       ./barapost.py -f reads.fastq -r prober_outdir\n
  2) Process FASTQ file and FASTA file with discoMegablast, packet size of 5 sequences.
     Files 'reads.fastq.gz' and 'another_sequences.fasta' have been already processed by 'prober.py'.
     Results of prober.py work are in directory 'prober_outdir':\n
       ./barapost.py -f reads.fastq.gz -f another_sequences.fasta -a discoMegablast -r prober_outdir\n
  3) Process all FASTQ and FASTA files in directory named "some_dir".
  All these files have been already processed by 'prober.py'.
  Results of prober.py work are in directory 'prober_outdir':\n
       ./barapost.py -d some_dir -r prober_outdir
"""
from sys import argv
import getopt

try:#{
    opts, args = getopt.getopt(argv[1:], "hf:d:o:p:a:r:l:o:t:",
        ["help", "infile=", "indir=", "packet-size=", "algorithm=", "prober-result-dir=",
        "local-fasta-to-bd=", "outdir=", "threads="])
#}
except getopt.GetoptError as gerr:#{
    print( str(gerr) )
    platf_depend_exit(2)
#}

is_fq_or_fa = lambda f: True if not re_search(r".*\.(m)?f(ast)?(a|q)(\.gz)?$", f) is None else False

# Default values:
fq_fa_list = list()
indir_path = None
packet_size = 100 # default
blast_algorithm = "megaBlast" # default
prober_res_dir = "prober_result" # default
your_own_fasta_lst = list()

if len(args) != 0:#{
    print_error("barapost.py does not take any positional arguments")
    print("Here are positional arguments specified by you:")
    for a in args:
        print(' ' + a)
    platf_depend_exit(1)
#}

if ("-o" in argv or "--outdir" in argv) and not ("-l" in argv or "--local-fasta-to-bd" in argv):#{
    print_error("'-o' option can't be specified without '-l' option!")
    print("For help type:\n python3 barapost.py -h")
    platf_depend_exit(1)
#}

for opt, arg in opts:#{
    
    if opt in ("-h", "--help"):#{
        print(help_msg)
        platf_depend_exit(0)
    #}

    if opt in ("-f", "--infile"):#{
        if not os.path.exists(arg):#{
            print_error("file '{}' does not exist!".format(arg))
            platf_depend_exit(1)
        #}
        if not is_fq_or_fa(arg):#{
            print_error("file '{}' is not '.fastq' or '.fasta'!".format(arg))
            platf_depend_exit(1)
        #}
        fq_fa_list.append( os.path.abspath(arg) )
    #}

    if opt in ("-d", "--indir"):#{
        if not os.path.exists(arg):#{
            print_error("directory '{}' does not exist!".format(arg))
            platf_depend_exit(1)
        #}
        if not os.path.isdir(arg):#{
            print_error("'{}' is not a directory!".format(arg))
            platf_depend_exit(1)
        #}
        indir_path = os.path.abspath(arg)

        fq_fa_list.extend(list( filter(is_fq_or_fa, os.listdir(indir_path)) ))
    #}

    if opt in ("-p", "--packet-size"):#{
        try:#{
            packet_size = int(arg)
            if packet_size < 1 or packet_size > 500:
                raise ValueError
        #}
        except ValueError:#{
            print_error("packet_size (-p option) must be integer number from 1 to 500")
            platf_depend_exit(1)
        #}
    #}

    if opt in ("-a", "--algorithm"):#{
        if not arg in ("megaBlast", "discoMegablast", "blastn"):#{
            print_error("invalid value specified by '-a' option!")
            print("Available values: 'megaBlast', 'discoMegablast', 'blastn'")
            platf_depend_exit(1)
        #}
        blast_algorithm = arg
    #}

    if opt in ("-r", "--prober-result-dir"):#{
        if not os.path.exists(arg):#{
            print_error("directory '{}' does not exist!".format(arg))
            platf_depend_exit(1)
        #}
        if not os.path.isdir(arg):#{
            print_error("'{}' is not a directory!".format(arg))
            platf_depend_exit(1)
        #}
        prober_res_dir = arg
    #}

    if opt in ("-l", "--local-fasta-to-bd"):#{

        if not os.path.exists(arg):#{
            print_error("fil '{}' does not exist!".format(arg))
            platf_depend_exit(1)
        #}

        your_own_fasta_lst.append(os.path.abspath(arg))
    #}

    if opt in ("-o", "--outdir"):#{
        prober_res_dir = arg
    #}

    elif opt in ("-t", "--threads"):#{
        try:#{
            n_thr = int(arg)
            if n_thr < 1:
                raise ValueError
        #}
        except ValueError:#{
            print_error("number of threads must be positive integer number!")
            print(" And here is your value: '{}'".format(arg))
            exit(1)
        #}
        if n_thr > mp.cpu_count():#{
            print("""\n  Warning! You have specified {} threads to use
    while {} are available.\n""".format(n_thr, mp.cpu_count()))
            reply = input("""If this is just what you want, press ENTER
    or enter 'q' to exit:>>""")
            if reply == "":#{
                pass
            #}
            else:#{
                exit(0)
            #}
        #}
    #}
#}

if not os.path.exists(prober_res_dir) and len(your_own_fasta_lst) == 0:#{
    print_error("file '{}' does not exist!".format(prober_res_dir))
    if prober_res_dir == "prober_result":
        print("""Maybe, output directory generated by 'prober.py' hasn't been named 'prober_result'
    and you have forgotten to specify '-r' option.""")
    platf_depend_exit(1)
#}


# If no FASTQ or FASTA file have been found
if len(fq_fa_list) == 0:#{
    # If input directory was specified -- exit
    if not indir_path is None:#{
        print_error("""no input FASTQ or FASTA files specified
    or there is no FASTQ and FASTA files in the input directory.\n""")
        platf_depend_exit(1)
    #}
    # If input directory was not specified -- look for FASTQ files in current directory
    else:#{
        fq_fa_list = list( filter(is_fq_or_fa, os.listdir('.')) )
        # Query files might be in current dicectory. They won't be processed:
        query_patt = r"^query[0-9]+\.fasta$"
        fq_fa_list = list( filter(lambda f: True if re_search(query_patt, f) is None else False, fq_fa_list))
        if len(fq_fa_list) == 0:#{
            print_error("there is no FASTQ or FASTA files to process found.")
            platf_depend_exit(1)
        #}
    #}
#}

del help_msg # we do not need it any more


# Check if 'blast+' tookit is installed
pathdirs = os.environ["PATH"].split(os.pathsep)

# Add '.exe' extention on order to find executables on Windows
if platform.startswith("win"):#{
	exe_ext = ".exe"
#}
else:#{
	exe_ext = ""
#}

for utility in ("blastn"+exe_ext, "makeblastdb"+exe_ext, "makembindex"+exe_ext):#{

    utility_found = False

    for directory in pathdirs:#{
        if os.path.exists(directory) and utility in os.listdir(directory):#{
            utility_found = True
            break
        #}
    #}

    if not utility_found:#{
        print("\tAttention!\n'{}' from blast+ toolkit is not found in your system.".format(utility))
        print("""If this error still occures although you have installed everything 
-- make sure that this program is added to PATH)""")
        platf_depend_exit(1)
    #}
#}


acc_fpath = os.path.join(prober_res_dir, "{}_probe_acc_list.tsv".format(blast_algorithm)) # form path to accession file

if not os.path.exists(acc_fpath) and len(your_own_fasta_lst) == 0:#{
    print_error("accession file '{}' not found!".format(acc_fpath))
    if len(your_own_fasta_lst) == 0:
        platf_depend_exit(1)
#}


print( get_work_time() + " ({}) ".format(strftime("%d.%m.%Y %H:%M:%S", localtime(start_time))) + "- Start working\n")


# |===== Function for checking if 'https://ncbi.nlm.nih.gov' is available =====|

def check_connection(address):#{
    """
    Function checks if 'https://ncbi.nlm.nih.gov' is available.

    :return: None if 'https://ncbi.nlm.nih.gov' is available;
    """

    try:#{

        ncbi_server = address
        status_code = urllib.request.urlopen(ncbi_server).getcode()

        # Just in case
        if status_code != 200:#{
            print('\n' + get_work_time() + " - Site '{}' is not available.".format(address))
            print("Check your Internet connection.\a")
            print("Status code: {}".format(status_code))
            platf_depend_exit(-2)
        #}
        return
    #}
    except OSError as err:#{

        print('\n' + get_work_time() + " - Site '{}' is not available.".format(address))
        print("Check your Internet connection.\a")
        print('\n' + '=/' * 20)
        print( str(err) )

        # 'urllib.request.HTTPError' can provide a user with information about the error
        if isinstance(err, HTTPError):#{
            print("Status code: {}".format(err.code))
            print(err.reason)
        #}
        platf_depend_exit(-2)
    #}
#}


# |===== Functionality for proper processing of gzipped files =====|

OPEN_FUNCS = (open, open_as_gzip)

is_gzipped = lambda file: True if file.endswith(".gz") else False
is_fastq = lambda f: True if not re_search(r".*\.fastq(\.gz)?$", f) is None else False

# Data from plain text and gzipped should be parsed in different way,
#   because data from .gz is read as 'bytes', not 'str'.
FORMATTING_FUNCS = (
    lambda line: line.strip(),   # format text line
    lambda line: line.decode("utf-8").strip()  # format gzipped line
)


# |=== Delimiter for result tsv file ===|
DELIM = '\t'


# |=== File format constants ===|
FASTQ_LINES_PER_READ = 4
FASTA_LINES_PER_SEQ = 2


def fastq2fasta(fq_fa_path, new_dpath):#{
    """
    Function conwerts FASTQ file to FASTA format, if there is no FASTA file with
        the same name as FASTQ file. Also it counts sequences in this file.

    :param fq_fa_path: path to FASTQ or FASTA file being processed;
    :type fq_fa_path: str;
    :param i: order number of fq_fa_path;
    :type i: int;
    :param new_dpath: path to current (corresponding to fq_fa_path file) result directory;
    :type new_dpath: str;

    Returns dict of the following structure:
    {
        "fpath": path_to_FASTA_file (str),
        "nreads": number_of_reads_in_this_FASTA_file (int)
    }
    """

    if not os.path.exists(new_dpath):#{
        try:#{
            os.makedirs(new_dpath)
        #}
        except OSError as err:#{
            print_error("error while creating directory '{}'".format(new_dpath))
            print( str(err) )
            platf_depend_exit(1)
        #}
    #}
    
    fasta_path = os.path.basename(fq_fa_path).replace(".fastq", ".fasta") # change extention
    fasta_path = fasta_path.replace(".gz", "") # get rid of ".gz" extention
    fasta_path = os.path.join(new_dpath, fasta_path) # place FASTA file into result directory

    how_to_open = OPEN_FUNCS[ is_gzipped(fq_fa_path) ]

    fastq_patt = r".*\.f(ast)?q(\.gz)?$"

    num_lines = 0 # variable for counting lines in a file
    if not re_search(fastq_patt, fq_fa_path) is None and not os.path.exists(fasta_path):#{

        # Get ready to process gzipped files
        how_to_open = OPEN_FUNCS[ is_gzipped(fq_fa_path) ]
        fmt_func = FORMATTING_FUNCS[ is_gzipped(fq_fa_path) ]

        with how_to_open(fq_fa_path) as fastq_file, open(fasta_path, 'w') as fasta_file:#{

            counter = 1 # variable for retrieving only 1-st and 2-nd line of FASTQ record
            for line in fastq_file:#{
                line = fmt_func(line)
                if counter <= 2:#{      write only 1-st and 2-nd line out of 4
                    if line[0] == '@':
                        line = '>' + line[1:]  # replace '@' with '>'
                    fasta_file.write(line + '\n')
                #}
                # reset the counter if the 4-th (quality) line has been encountered
                elif counter == 4:
                    counter = 0
                counter += 1
                num_lines += 1
            #}
        #}
        num_reads = int(num_lines / FASTQ_LINES_PER_READ) # get number of sequences

        print("\n'{}' ({} reads) --> FASTA".format(os.path.basename(fq_fa_path), num_reads))
    #}
    # IF FASTA file is already created
    # We need only number of sequences in it.
    elif not re_search(fastq_patt, fq_fa_path) is None and os.path.exists(fasta_path):#{
        num_lines = sum(1 for line in how_to_open(fq_fa_path)) # get number of lines
        num_reads = int( num_lines / FASTQ_LINES_PER_READ ) # get number of sequences
    #}
    # If we've got FASTA source file
    # We need only number of sequences in it.
    else:#{
        # num_lines = sum(1 for line in how_to_open(fq_fa_path)) # get number of lines
        # num_reads = int( num_lines / FASTA_LINES_PER_SEQ ) # get number of sequences
        num_reads = sum(1 if line[0] == '>' else 0 for line in how_to_open(fq_fa_path, 'r'))
        # If we've got FASTA source file we do not need to copy it
        fasta_path = fq_fa_path
    #}

    return {"fpath": fasta_path, "nreads": num_reads}
#}


def rename_file_verbosely(file, directory):#{
    is_analog = lambda f: file[file.rfind('.')] in f
    num_analog_files = len( list(filter(is_analog, os.listdir(directory))) )

    if os.path.isdir(file):
        word = "directory"
    else:
        word = "file"

    try:#{
        print('\n' + get_work_time() + " - Renaming old {}:".format(word))
        name_itself = file[: file.rfind('.')]
        ext = file[file.rfind('.'):]
        num_analog_files = str(num_analog_files)
        new_name = name_itself+"_old_"+num_analog_files+ext
        print("\t'{}' --> '{}'".format(os.path.basename(file), new_name))
        os.rename(file, new_name)
    #}
    except Exception as err:#{
        # Anything (and not only strings) can be passed to the function
        print("\n {} '{}' cannot be renamed:".format( word, str(file)) )
        print( str(err) + '\n')
        platf_depend_exit(1)
    #}
#}


def look_around(new_dpath, fasta_path, blast_algorithm):#{
    """
    Function looks around in order to ckeck if there are results from previous runs of this script.

    Returns None if there is no result from previous run.
    If there are results from previous run, returns a dict of the following structure:
    {
        "pack_size": packet_size (int),
        "tsv_respath": path_to_tsv_file_from_previous_run (str),
        "n_done_reads": number_of_successfull_requests_from_currenrt_FASTA_file (int),
        "tmp_fpath": path_to_pemporary_file (str)
    }

    :param new_dpath: path to current (corresponding to fq_fa_path file) result directory;
    :type new_dpath: str;
    :param fasta_path: path to current (corresponding to fq_fa_path file) FASTA file;
    :type fasta_path: str;
    :param blast_algorithm: BLASTn algorithm to use.
        This parameter is necessary because it is included in name of result files;
    :type blast_algorithm: str;
    """

    # "hname" means human readable name (i.e. without file path and extention)
    fasta_hname = os.path.basename(fasta_path) # get rid of absolute path
    fasta_hname = re_search(r"(.*)\.(m)?f(ast)?a", fasta_hname).group(1) # get rid of '.fasta' extention

    # Form path to temporary file
    tmp_fpath = "{}_{}_temp.txt".format(os.path.join(new_dpath, fasta_hname), blast_algorithm)
    # Form path to result file
    tsv_res_fpath = "{}_{}_result.tsv".format(os.path.join(new_dpath, fasta_hname), blast_algorithm)

    num_done_reads = None # variable to keep number of succeffdully processed sequences

    if os.path.exists(tsv_res_fpath):#{
        with open(tsv_res_fpath, 'r') as res_file:#{
            try:#{ There can be invalid information in result file
                lines = res_file.readlines()
                num_done_reads = len(lines) - 1 # the first line is a head
                last_line = lines[-1]
                last_seq_id = last_line.split(DELIM)[0]
            #}
            except Exception as err:#{
                print("\nData in result file '{}' is broken.".format(tsv_res_fpath))
                print( str(err) )
                print("Start from the beginning.")
                rename_file_verbosely(tsv_res_fpath, new_dpath)
                rename_file_verbosely(tmp_fpath, new_dpath)
                return None
            #}
            else:#{
                print("Last processed sequence: " + last_seq_id)
            #}
        #}
    #}

    # If we start from the beginning, we have no sequences processed
    if num_done_reads is None:
        num_done_reads = 0

    global packet_size # use default value at first

    try:#{ There can be invalid information in tmp file of tmp file may not exist
        with open(tmp_fpath, 'r') as tmp_file:
            temp_line = tmp_file.readline()
        packet_size = int(re_search(r"packet_size: {}", temp_line).group(1).strip())
        print("\nPacket size switched to '{}', as it was during previous run".format(packet_size))
    #}
    except Exception as err:#{
        pass
    #}
    finally:#{
        # Return data from previous run
        return {
            "pack_size": packet_size,
            "tsv_respath": tsv_res_fpath,
            "n_done_reads": num_done_reads,
            "tmp_fpath": tmp_fpath
        }
    #}
#}


def pass_processed_seqs(fasta_file, num_done_reads, fmt_func):#{
    if num_done_reads == 0:
        return None
    else:#{
        i = 0
        while i <= num_done_reads:#{
            line = fmt_func(fasta_file.readline())
            if line .startswith('>'):#{
                if ' ' in line:
                    line = line.partition(' ')[0]+'\n'
                next_id_line = line
                i += 1
            #}
        #}
        return next_id_line
    #}
#}


def fasta_packets(fasta, packet_size, reads_at_all, num_done_reads):#{

    # if 'fasta' is a path to FASTA file
    if not '\n' in r"{}".format(fasta):#{
    
        if is_gzipped(fasta):#{
            fmt_func = lambda l: l.decode("utf-8")
            how_to_open = open_as_gzip
        #}
        else:#{
            fmt_func = lambda l: l
            how_to_open = open
        #}

        fasta_file = how_to_open(fasta)
        get_next_line = lambda: fmt_func(fasta_file.readline())
    #}
    # if 'fasta' is actual FASTA data
    else:#{

        fasta_lines = fasta.splitlines()
        line_i = 0

        def get_next_line():#{
            try:#{
                line = fasta_lines[i]
            #}
            except IndexError:#{
                # if IndexError is rised -- we have reached the end of data
                # Simulate returning of empty string, just like io.TextIOWrapper.readline() does
                #    if end of file is reached:
                return ""
            #}
            else:#{
                line_i += 1
                return line
            #}
        #}
    #}

    # Variable that contains id of next sequence in current FASTA file.
    # If no or all sequences in current FASTA file have been already processed, this variable is None
    next_id_line = pass_processed_seqs(fasta_file, num_done_reads, fmt_func)

    line = get_next_line()
    if line.startswith('>') and ' ' in line:#{:
        line = line.partition(' ')[0]+'\n'
    #}

    packet = ""
    if not next_id_line is None:
        packet += next_id_line
    packet += line

    packs_at_all = reads_at_all // packet_size # Calculate total number of packets sent from current FASTA file
    if reads_at_all % packet_size > 0: # And this is ceiling (in order not to import 'math')
        packs_at_all += 1
    packs_processed = int( num_done_reads / packet_size ) # number of successfully processed sequences

    reads_left = reads_at_all - num_done_reads # number of sequences left to process
    packs_left = packs_at_all - packs_processed # number of packets left to send
    pack_to_send = packs_processed+1 if packs_processed > 0 else 1 # number of packet meant to be sent now

    # Iterate over packets left to process
    for _ in range(packs_left):#{

        # if not next_id_line is None:
        #     packet += next_id_line
        i = 0
        
        while i < packet_size:#{

            line = get_next_line()
            if line.startswith('>'):#{
                if ' ' in line:
                    line = line.partition(' ')[0]+'\n'
                i += 1
            #}
            if line == "":
                break
            packet += line
        #}

        if line != "":#{
            next_id_line = packet.splitlines()[-1]+'\n'
            packet = '\n'.join(packet.splitlines()[:-1])
        #}
        else:#{
            next_id_line = None
        #}

        names = list( filter(lambda l: True if l.startswith('>') else False, packet.splitlines()) )
        names = list( map(lambda l: l.partition(' ')[0].strip(), names) )

        if packet is "":#{   Just in case
            print("Recent packet is empty")
            return
        #}

        yield {"fasta": packet.strip(), "names": names}
        if not next_id_line is None:
            packet = next_id_line+'\n'
        else:
            return
    #}
    fasta_file.close()
#}


# Variable for counting accessions of records menat to be downloaded from Genbank.
# Is used only for printing the list of accessions to console.
acc_counter = 0

import threading # There is no need to import this module if '--remote-only' is specified.


def get_gi_by_acc(acc):#{

    global acc_dict

    try:#{
        gi_num = acc_dict[acc][0]
    #}
    except KeyError:#{
        print_error("GI number error. Please, contact the developer")
        platf_depend_exit(1)
    #}
    return gi_num
#}


def retrieve_fastas_by_gi(gi_list, db_dir):#{
    """
    Function downloads set of records from Genbank according to list of GIs passed to it.
    Downloaded FASTA file will be placed in 'db_dir' directory and named 'local_seq_set.fasta'

    :param gi_list: list of GI numbers of sequences meant to be downloaded;
    :type gi_list: list<str>;
    :param db_dir: path to directory in which downloaded FASTA file will be placed;
    :type db_dir: str;

    Returns path to downloaded FASTA file of 'str'.
    """

    local_fasta = os.path.join(db_dir, "local_seq_set.fasta") # path to downloaded FASTA file
    if len(gi_list) == 0:
        return local_fasta

    gis_del_comma = ','.join(gi_list) # GI numbers must be separated by comma in url
    # E-utilities provide us with possibility of downloading records from Genbank by GI numbers.
    retrieve_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=nuccore&id={}&rettype=fasta&retmode=text".format(gis_del_comma)
    
    global stop_wait # a flag variable that will signal waiter-function to stop executing

    def download_waiter():#{
        """
        Function waits untill 'local_fasta' file is downloaded.
        It prints size of downloaded data to console during downloading.
        This function just waits -- it won't bring you the menu :).
        """
        # Wait untill downloading starts
        while not os.path.exists(local_fasta):
            if stop_wait:
                return
            sleep(1)

        while not stop_wait:#{
            # Get size of downloaded data
            fsize = round(os.path.getsize(local_fasta) / (1024**2), 1) # get megabytes
            printn("\r{} - {} MB downloaded ".format(get_work_time(), fsize))
            sleep(1) # instant updates are not necessary
        #}
        # Print total size of downloaded file
        fsize = round(os.path.getsize(local_fasta) / (1024**2), 1)
        print("\r{} - {} MB downloaded \n".format(get_work_time(), fsize))
    #}

    error = True
    while error:#{
        try:#{
            waiter = threading.Thread(target=download_waiter) # create thread
            stop_wait = False # raise the flag
            waiter.start() # start waiting
            
            print("\n{} - Downloading sequences for local database building started".format(get_work_time()))

            urllib.request.urlretrieve(retrieve_url, local_fasta) # retrieve FASTA file
        #}
        except Exception as err:#{
            print_error("error while downloading FASTA files")
            print( str(err) )
            print("'barapost.py' will try again in 30 seconds")
            os.unlink(local_fasta)
            sleep(30)
        #}
        else:#{
            error = False
        #}
        finally:#{
            stop_wait = True # lower the flag
            waiter.join() # main thread will wait until waiter function ends it's work
        #}
    #}

    print("{} - Downloading is completed".format(get_work_time()))

    return local_fasta
#}


def build_local_db(acc_dict, prober_res_dir):#{
    """
    Function builds a local indexed database with utilities from 'blast+' toolkit.

    :param acc_dict: a dictionary of accessions and record names
        Accession are keys, record names are values;
    :type acc_dict: dict<str, str>;
    :param prober_res_dir: path to current result directory (each processed file has it's own result directory);
    :type prober_res_dir: str;

    Returns path to builded local indexed database.
    """

    db_dir = os.path.join(prober_res_dir, "local_database") # path to directory in which database will be placed
    try:#{
        os.makedirs(db_dir)
    #}
    except OSError as err:#{
        #If this directory exists

        while True:#{

            print("Database directory exists.")
            if len(os.listdir(db_dir)) == 0:#{
                # If it is empty -- nothing stops us. break and build a database
                print("It is empty. Building a database...")
                break
            #}
            else:#{
                # If there are some files, the user will decide, whether to use this database
                # or to remove it and to build again (e.g. if the database haven't been builded successfully) .
                print("Here are files located in this directory:")
                for i, file in enumerate(os.listdir(db_dir)):
                    print("  {}. '{}'".format(i+1, file))

                reply = input("""\nPress ENTER to continue aligning using this database.
Enter 'r' to remove all files in this directory and build the database from the beginning:>>""")

                if reply == "":
                    # Do not build a database, just return path to it.
                    return os.path.join(db_dir, "local_seq_set.fasta")
                elif reply == 'r':
                    # Empty this directory and break from the loop in order to build a database.
                    for file in os.listdir(db_dir):#{
                        if os.path.exists(file):#{
                            os.unlink(file)
                        #}
                    #}

                    # Find directories with results left from using old database (they will be renamed):
                    old_dirs = list( filter(lambda f: True if os.path.isdir(f) and f != "local_database" else False, os.listdir(prober_res_dir)) )
                    if len(old_dirs) > 0:#{
                        print("\n Directories with results of using old database are found")
                        print("Renaming them...")
                        for directory in old_dirs:
                            rename_file_verbosely(directory, prober_res_dir)
                    #}
                    break
                else:
                    # Ask again
                    continue
            #}
        #}
    #}

    # If accession file does not exist and execution has reached here -- everything is OK --
    #    we are building local database from local files only.
    if os.path.exists(acc_fpath):#{
        print("Checking Internet connection...")
        check_connection("https://ncbi.nlm.nih.gov")
        print("OK\n")

        print("""Following sequences will be downloaded from Genbank
    for further aligning on your local machine with 'blast+' toolkit:\n""")
        for i, acc in enumerate(acc_dict.keys()):#{
            print(" {}. {} - '{}'".format(i+1, acc, acc_dict[acc][1]))
        #}
        print()
    #}
    
    # Get list of GI numbers. Function 'get_gi_by_acc' will print the list of GIs to console.
    gi_list = list( map(get_gi_by_acc, acc_dict.keys()) )

    local_fasta = retrieve_fastas_by_gi(gi_list, db_dir) # download FASTA file

    # Add 'your own' FASTA files to database
    if not len(your_own_fasta_lst) == 0:#{

        # This variable counts sequences from local files.
        # It is necessary for accession deduplication.
        own_seq_counter = 0

        # Check if these files are SPAdes of a5 assembly
        spades_patt = r"NODE_[0-9]+" # this pattern will match sequence IDs generated y SPAdes
        spades_counter = 0 # variable counts number of SPAdes assembly files
        spades_assms = list() # this list will contain paths to SPAdes assembly files
        a5_patt = r"scaffold_[0-9]+" # this pattern will match sequence IDs generated y a5
        a5_counter = 0 # variable counts number of a5 assembly files
        a5_assms = list() # this list will contain paths to a5 assembly files

        for own_fasta_path in your_own_fasta_lst:#{

            how_to_open = OPEN_FUNCS[ is_gzipped(own_fasta_path) ]

            with how_to_open(own_fasta_path) as fasta_file:
                first_seq_id = fasta_file.readline().strip() # get the first line in file (the first seq ID)

            if not re_search(spades_patt, first_seq_id) is None:#{ if we've got SPAdes assembly
                spades_counter += 1
                spades_assms.append(own_fasta_path)
                continue
            #}
            if not re_search(a5_patt, first_seq_id) is None:#{  if we've got SPAdes assembly
                a5_counter += 1
                a5_assms.append(own_fasta_path)
                continue
            #}
        #}

        for counter, assm_lst in zip((spades_counter, a5_counter), (spades_assms, a5_assms)):#{

            # If there are more than one file with assembly of one assembler,
            #    we need to distinguish these files (i.e. these assemblies).
            if counter > 1:#{

                # Remove this files from list -- they will be processed in a specific way
                for file in assm_lst:#{
                    your_own_fasta_lst.remove(file)
                #}

                # If there are any equal basenames of files, absolute paths to these files will be used in seq IDs:
                assm_basenames = list( map(os.path.basename, assm_lst) ) # get basenames

                # If this sum is equal to length of 'assm_basenames' -- there are no duplicated basenames.
                # So, there is no need to use absolute paths.# Absolute path will be used otherwise.
                dedpul_sum = sum( map(assm_basenames.count, assm_basenames) )

                # Path conversion according to 'deduplication sum':
                if dedpul_sum == len(assm_basenames):#{
                    assm_lst = list( map(os.path.basename, assm_lst) )
                #}
                else:#{
                    assm_lst = list( map(os.path.abspath, assm_lst) )
                #}

                # Add assembled sequences to database
                fasta_db = open(local_fasta, 'a')
                for assm_path in assm_lst:#{
                    print("{} - Adding '{}' to database...".format(get_work_time(), os.path.basename(assm_path)))

                    how_to_open = OPEN_FUNCS[ is_gzipped(assm_path) ]
                    with how_to_open(assm_path) as fasta_file:#{
                        for line in fasta_file:#{
                            line = line.strip()
                            # You can find comments to "OWN_SEQ..." below. I don't want to duplicate them.
                            # Paths will be written to seq IDs in following way:
                            #   (_/some/happy/path.fastq_)
                            # in order to retrieve them securely with regex later.
                            if line.startswith('>'):#{
                                own_seq_counter += 1
                                line = ">" + "OWN_SEQ_{} (_{}_)_".format(own_seq_counter, assm_path) + line[1:]
                            #}
                            fasta_db.write(line + '\n')
                        #}
                    #}
                #}
                fasta_db.close()
            #}
        #}

        # No 'with open' here in order not to indent too much.
        fasta_db = open(local_fasta, 'a')
        for own_fasta_path in your_own_fasta_lst:#{
            print("{} - Adding '{}' to database...".format(get_work_time(), os.path.basename(own_fasta_path)))

            how_to_open = OPEN_FUNCS[ is_gzipped(own_fasta_path) ]
            with how_to_open(own_fasta_path) as fasta_file:#{
                for line in fasta_file:#{
                    line = line.strip()
                    # 'makeblastdb' considers first word (sep. is space) as sequence ID
                    #   and throws an error if there are duplicate IDs.
                    # In order not to allow this duplication we'll create our own sequence IDs:
                    #   'OWN_SEQ_<NUMBER>' and write it in the beginning of FASTA record name.
                    if line.startswith('>'):#{
                        own_seq_counter += 1
                        line = ">" + "OWN_SEQ_{} ".format(own_seq_counter) + line[1:]
                    #}
                    fasta_db.write(line + '\n')
                #}
            #}
        #}
        fasta_db.close()
    #}

    # Configure command line
    make_db_cmd = "makeblastdb -in {} -parse_seqids -dbtype nucl".format(local_fasta)
    exit_code = os.system(make_db_cmd) # make a blast-format database
    if exit_code != 0:#{
        print_error("error while making the database")
        platf_depend_exit(exit_code)
    #}
    print("""{} - Database is successfully created:
'{}'\n""".format(get_work_time(), local_fasta))

    print("{} - Database index creating started".format(get_work_time()))
    # Configure command line
    make_index_cmd = "makembindex -input {} -iformat blastdb -verbosity verbose".format(local_fasta)
    exit_code = os.system(make_index_cmd) # create an index for the database
    if exit_code != 0:#{
        print_error("error while creating database index")
        platf_depend_exit(exit_code)
    #}
    print("{} - Database index has been successfully created\n".format(get_work_time()))

    # Gzip downloaded FASTA file in order to save space on disk
    print("Gzipping downloaded FASTA file:\n '{}'".format(local_fasta))

    # GNU gzip utility is faster, but there can be absence of it
    gzip_util = "gzip"
    util_found = False
    for directory in os.environ["PATH"].split(os.pathsep):#{
        if os.path.isdir(directory) and gzip_util in os.listdir(directory):#{
            util_found = True
            break
        #}
    #}
    if util_found:#{
        os.system("{} {}".format(gzip_util, local_fasta))
    #}
    else:#{
        with open(local_fasta, 'r') as fasta_file, open_as_gzip(local_fasta+".gz", "wb") as fagz_file:#{
            for line in fasta_file:
                fagz_file.write(bytes(fasta_file.readline(), "utf-8"))
        #}
        os.unlink(local_fasta) # remove source FASTA file, not the database
    #}
    print() # just print blank line

    return local_fasta
#}


def launch_blastn(packet, blast_algorithm):#{
    """
    Function meant to replace 'configure_request' one that works with BLAST server.
    """

    # Algorithms in 'blast+' are named in a little different way comparing to BLAST server.
    if blast_algorithm == "megaBlast":
        blast_algorithm = "megablast"
    elif blast_algorithm == "discoMegablast":
        blast_algorithm = "dc-megablast"

    # Indexed discontiguous searches are not supported:
    # https://www.ncbi.nlm.nih.gov/books/NBK279668/#usermanual.Megablast_indexed_searches
    if blast_algorithm != "dc-megablast":
        use_index = "true"
    else:
        use_index = "false"

    # PID of current process won't change, so we can use it to mark query files.
    # 'paket's are too large to pass them to 'subprocess.Popen' as stdin,
    #    therefore we need to use these query files.
    query_path = "query{}.fasta".format(os.getpid())

    with open(query_path, 'w') as query_file:#{
        query_file.write(packet)
    #}

    # Configure command line
    blast_cmd = "blastn -query {} -db {} -outfmt 5 -task {} -max_target_seqs 1 -use_index {}".format(query_path,
        local_fasta, blast_algorithm, use_index)

    pipe = sp_Popen(blast_cmd, shell=True, stdout=sp_PIPE, stderr=sp_PIPE)
    stdout_stderr = pipe.communicate()

    if pipe.returncode != 0:#{
        print_error("error while aligning a sequence against local database")
        print(stdout_stderr[1].decode("utf-8"))
        exit(pipe.returncode)
    #}

    return stdout_stderr[0].decode("utf-8")
#}


def remove_tmp_files(*paths):#{
    """
    Function removes files passed to it.
    Actually, passed arguments are paths ('str') to files meant to be removed.
    """
    for path in paths:#{
        if os.path.exists(path):#{
            os.unlink(path)
        #}
    #}
#}


get_phred33 = lambda q_symb: ord(q_symb) - 33

def get_read_avg_qual(qual_str):#{

    phred33 = map(get_phred33, list(qual_str))
    read_qual = round( sum(phred33) / len(qual_str), 2 )
    return read_qual
#}

def configure_qual_dict(fastq_path):#{

    qual_dict = dict()
    how_to_open = OPEN_FUNCS[ is_gzipped(fastq_path) ]
    fmt_func = FORMATTING_FUNCS[ is_gzipped(fastq_path) ]

    with how_to_open(fastq_path) as fastq_file:#{
        counter = 1
        line = fmt_func(fastq_file.readline())
        while line != "":#{
            if counter == 1:#{
                seq_id = intern( (line.partition(' ')[0])[1:] )
            #}
            counter += 1
            line = fmt_func(fastq_file.readline())
            if counter == 4:#{
                qual_dict[seq_id] = get_read_avg_qual(line)
                counter = 0
            #}
        #}
    #}

    return qual_dict
#}


def parse_align_results_xml(xml_text, seq_names, qual_dict):#{
    """
    Function parses BLAST xml response and returns tsv lines containing gathered information:
        1. Query name.
        2. Hit name formatted by 'format_taxonomy_name()' function.
        3. Hit accession.
        4. Length of alignment.
        5. Percent of identity.
        6. Percent of gaps.
        7. E-value.
    Erroneous tsv lines that function may produce:
        1. "<query_name>\\tQuery has been lost: ERROR, Bad Gateway"
            if data packet has been lost.
        2. "<query_name>\\tQuery has been lost: BLAST ERROR"
            if BLAST error occured.
        3. "<query_name>\\tNo significant similarity found"
            if no significant similarity has been found
        Type of return object: list<str>.
    """

    result_tsv_lines = list()

    # /=== Validation ===/

    if "Bad Gateway" in xml_text:#{
        print('\n' + '=' * 45)
        print(get_work_time() + " - ERROR! Bad Gateway! Data from last packet has lost.")
        print("It would be better if you restart the script.")
        print("Here are names of lost queries:")
        for i, name in enumerate(seq_names):#{
            print("{}. '{}'".format(i+1, name))
            result_tsv_lines.append(name + DELIM + "Query has been lost: ERROR, Bad Gateway")
        #}
        input("Press ENTER to continue...")

        return result_tsv_lines
    #}

    if "to start it again" in xml_text:#{
        print('\n' + get_work_time() + "BLAST ERROR!")

        print("Here are names of lost queries:")
        for i, name in enumerate(seq_names):#{
            print("{}. '{}'".format(i+1, name))
            result_tsv_lines.append(name + DELIM +"Query has been lost: BLAST ERROR")
        #}

        input("Press ENTER to continue...")

        return result_tsv_lines
    #}

    # /=== Parse BLAST XML response ===/

    root = ElementTree.fromstring(xml_text) # get tree instance

    # Iterate through "Iteration" and "Iteration_hits" nodes
    for iter_elem, iter_hit in zip(root.iter("Iteration"), root.iter("Iteration_hits")):#{
    
        # "Iteration" node contains query name information
        query_name = iter_elem.find("Iteration_query-def").text

        query_len = iter_elem.find("Iteration_query-len").text

        if not qual_dict is None:#{
            ph33_qual = qual_dict[query_name]
            miscall_prop = round(10**(ph33_qual/-10), 3)
            accuracy = round( 100*(1 - miscall_prop), 2 ) # expected percent of correctly called bases
        #}
        else:#{
            # If FASTA file is processing, print dashed in quality columns
            ph33_qual = "-"
            accuracy = "-" # expected percent of correctly called bases
        #}

        # If there are any hits, node "Iteration_hits" contains at least one "Hit" child
        hit = iter_hit.find("Hit")
        if hit is not None:#{

            # Get full hit name (e.g. "Erwinia amylovora strain S59/5, complete genome")
            hit_name = hit.find("Hit_def").text
            # Format hit name (get rid of stuff after comma)
            hit_taxa_name = hit_name[: hit_name.find(',')] if ',' in hit_name else hit_name
            hit_taxa_name = hit_taxa_name.replace(" complete genome", "") # sometimes there are no comma before it
            hit_taxa_name = hit_taxa_name.replace(' ', '_')


            hit_acc = hit.find("Hit_accession").text # get hit accession

            # Find the first HSP (we need only the first one)
            hsp = next(hit.find("Hit_hsps").iter("Hsp"))

            align_len = hsp.find("Hsp_align-len").text.strip()

            pident = hsp.find("Hsp_identity").text # get number of matched nucleotides

            gaps = hsp.find("Hsp_gaps").text # get number of gaps

            evalue = hsp.find("Hsp_evalue").text # get e-value
            # If E-value is low enough -- add this subject sequence to 'acc_dict' to further downloading

            pident_ratio = round( float(pident) / int(align_len) * 100, 2)
            gaps_ratio = round( float(gaps) / int(align_len) * 100, 2)

            # Append new tsv line containing recently collected information
            result_tsv_lines.append( DELIM.join( (query_name, hit_taxa_name, hit_acc, query_len,
                align_len, pident, gaps, evalue, str(ph33_qual), str(accuracy)) ))
        #}
        else:
            # If there is no hit for current sequence
            result_tsv_lines.append(DELIM.join( (query_name, "No significant similarity found", "-", query_len,
                "-", "-", "-", "-", str(ph33_qual), str(accuracy)) ))
    #}
    return result_tsv_lines
#}


def write_result(res_tsv_lines, tsv_res_path):#{
    """
    Function writes result of blasting to result tsv file.

    :param res_tsv_lines: tsv lines returned by 'parse_align_results_xml()' funciton;
    :type res_tsv_lines: list<str>;
    :param tsv_res_path: path to reslut tsv file;
    :type tsv_res_path: str;
    """


    # If there is no result tsv fil -- create it and write a head of the table.
    if not os.path.exists(tsv_res_path):#{
        with open(tsv_res_path, 'w') as tsv_res_file:#{
            tsv_res_file.write(DELIM.join( ["QUERY_ID", "HIT_NAME", "HIT_ACCESSION", "QUERY_LENGTH",
                "ALIGNMENET_LENGTH", "IDENTITY", "GAPS", "E-VALUE", "AVG_PHRED33", "ACCURACY(%)"] ) + '\n')
        #}
    #}
    # Write reslut tsv lines to this file
    with open(tsv_res_path, 'a') as tsv_res_file:#{
        for line in res_tsv_lines:
            tsv_res_file.write(line + '\n')
    #}
#}

def get_curr_res_dir(fq_fa_path, prober_res_dir):#{
    
    # dpath means "directory path"
    new_dpath = os.path.join(prober_res_dir, os.path.basename(fq_fa_path)) # get rid of absolute path
    new_dpath = re_search(r"(.*)\.(m)?f(ast)?(a|q)", new_dpath).group(1) # get rid of extention

    return new_dpath
#}

def configure_acc_dict(acc_fpath):#{

    acc_dict = dict()
    global your_own_fasta_lst

    # if database will be builded only from 'your own' FASTA files -- return empty dict
    if not os.path.exists(acc_fpath):
        return acc_dict

    with open(acc_fpath, 'r') as acc_file:#{
        lines = acc_file.readlines()

        for line in lines:#{
            line = line.strip()
            if line != "" and not line.startswith('#') and not line.startswith("ACCESSION"):#{

                # Handle situation if user writes path to his own FASTA file
                #  (that is meant to be added to DB) to accession file.
                if not os.path.exists(line) and not re_search(r".*\.(m?)f(ast)?a(\.gz?)", line):#{
                    try:#{
                        line_splt = line.split(DELIM)
                        acc = intern(line_splt[0])
                        gi = line_splt[1]
                        name = line_splt[2]
                        acc_dict[acc] = (gi, name)
                    #}
                    except IndexError as inderr:#{
                        print_error("invalid data in file '{}'!".format(acc_fpath))
                        print("Seems, you have written path to file that does not exist or not a FASTA file.")
                        print("Here is this invalid line:\n   '{}'".format(line))
                        platf_depend_exit(1)
                    #}
                #}
                else:#{
                    your_own_fasta_lst.append(line)
                #}
            #}
        #}
    #}

    if len(acc_dict) == 0:#{
        print_error("no accession information found in file '{}".format(acc_fpath))
        platf_depend_exit(1)
    #}

    return acc_dict
#}


def spread_files_equally(n_thr, fq_fa_list):#{
    sublist_size = len(fq_fa_list) // n_thr
    if len(fq_fa_list) % n_thr != 0:
        sublist_size += 1

    i = 0
    sublist = list()
    for i, path in enumerate(fq_fa_list):#{

        sublist.append(path)

        if i+1 == sublist_size:
            yield sublist
            sublist = list()

        if i+1 == len(fq_fa_list):
            yield sublist
            return
    #}
#}


def init_proc_many_files(print_lock_buff):#{
    global print_lock
    print_lock = print_lock_buff
#}


def process_multiple_files(fq_fa_list, parallel=False):#{

    # Iterate over source FASTQ and FASTA files
    for i, fq_fa_path in enumerate(fq_fa_list):#{

        # Configure quality dictionary
        qual_dict = configure_qual_dict(fq_fa_path) if is_fastq(fq_fa_path) else None

        # Create the result directory with the name of FASTQ of FASTA file being processed:
        new_dpath = get_curr_res_dir(fq_fa_path, prober_res_dir)

        # Convert FASTQ file to FASTA (if it is FASTQ) and get it's path and number of sequences in it:
        curr_fasta = fastq2fasta(fq_fa_path, new_dpath)

        if parallel:
            with print_lock:
                print("\n '{}' ({} sequences) is processing".format(os.path.basename(fq_fa_path), curr_fasta["nreads"]))
        else:
            print("\n {}. '{}' ({} sequences) is processing".format(i+1, os.path.basename(fq_fa_path), curr_fasta["nreads"]))

        # "hname" means human readable name (i.e. without file path and extention)
        fasta_hname = os.path.basename(curr_fasta["fpath"]) # get rid of absolure path
        fasta_hname = re_search(r"(.*)\.(m)?f(ast)?a", fasta_hname).group(1) # get rid of file extention

        # Look around and ckeck if there are results of previous runs of this script
        # If 'look_around' is None -- there is no data from previous run
        previous_data = look_around(new_dpath, curr_fasta["fpath"],
            blast_algorithm)

        if previous_data is None:#{ # If there is no data from previous run

            num_done_reads = 0 # number of successfully processed sequences
            tsv_res_path = "{}_{}_result.tsv".format(os.path.join(new_dpath,
                fasta_hname), blast_algorithm) # form result tsv file path
            tmp_fpath = "{}_{}_temp.txt".format(os.path.join(new_dpath,
                fasta_hname), blast_algorithm) # form temporary file path
        #}
        else:#{ # if there is data from previous run

            num_done_reads = previous_data["n_done_reads"] # get number of successfully processed sequences
            packet_size = previous_data["pack_size"] # packet size sholud be the same as it was in previous run
            tsv_res_path = previous_data["tsv_respath"] # result tsv file sholud be the same as during previous run
            tmp_fpath = previous_data["tmp_fpath"] # temporary file sholud be the same as during previous run
        #}

        if parallel:
            print_lock.acquire()
        if num_done_reads > 0:#{
            verb, s_letter = ("have", "s") if num_done_reads != 1 else ("has", "")
            print("{} sequence{} {} been already processed.".format(num_done_reads, s_letter, verb))
        #}
        print("Writing results to file:\n '{}'".format(tsv_res_path))
        if parallel:
            print_lock.release()

        packs_at_all = (curr_fasta["nreads"] - num_done_reads) // packet_size # Calculate total number of packets sent from current FASTA file
        if (curr_fasta["nreads"] - num_done_reads) % packet_size != 0: # And this is ceiling (in order not to import 'math')
            packs_at_all += 1
        packs_processed = int( num_done_reads / packet_size ) # number of successfully processed sequences
        packs_left = packs_at_all - packs_processed

        if not parallel:
            printn("\r{} - (0/{}) sequence packets processed ".format(get_work_time(), packs_at_all))
        else:
            next_done_perc = 0.10
            perc_step = 0.10

        for pack_i, packet in enumerate(fasta_packets(curr_fasta["fpath"],
            packet_size, curr_fasta["nreads"], num_done_reads)):#{

            with open(tmp_fpath, 'w') as tmp_file:
                tmp_file.write("packet_size: {}".format(packet_size))

            # Align the packet
            align_xml_text = launch_blastn(packet["fasta"], blast_algorithm)

            # Get result tsv lines
            result_tsv_lines = parse_align_results_xml(align_xml_text,
                packet["names"], qual_dict)

            # Write the result to tsv
            write_result(result_tsv_lines, tsv_res_path)

            if not parallel:#{
                printn("\r{} - ({}/{}) sequence packets processed ".format(get_work_time(), pack_i+1, packs_at_all))
            #}
            elif parallel and (pack_i+1) / packs_left > next_done_perc:#{
                next_done_perc += perc_step
                with print_lock:#{
                    print("'{}' - {}% done".format( os.path.basename(fq_fa_path),
                        round(100.0 * (pack_i+1) / packs_left) ))
                #}
            #}
        #}
        if not parallel:
            print() # just print blank line
        remove_tmp_files(tmp_fpath)
    #}
    remove_tmp_files("query{}.fasta".format(os.getpid()))
#}


def init_proc_single_file_in_paral(print_lock_buff, write_lock_buff, pack_i_buff, pack_i_lock_buff):#{
    
    global print_lock
    print_lock = print_lock_buff

    global write_lock
    write_lock = write_lock_buff

    global pack_i
    pack_i = pack_i_buff

    global pack_i_lock
    pack_i_lock = pack_i_lock_buff

#}


def process_part_of_file(data, tsv_res_path, qual_dict, packs_at_all):#{

    # This character cannot appear anywhere accept the begining of sequence ID in FASTA file,
    #    therefor it is a valid check:
    seqs_at_all = data.count('>')
    
    for packet in fasta_packets(fasta=data, packet_size=packet_size,
            reads_at_all=seqs_at_all, num_done_reads=0):#{

        # Align the packet
        align_xml_text = launch_blastn(packet["fasta"], blast_algorithm)

        # Get result tsv lines
        result_tsv_lines = parse_align_results_xml(align_xml_text,
            packet["names"], qual_dict)

        # Write the result to tsv
        with write_lock:
            write_result(result_tsv_lines, tsv_res_path)

        with pack_i_lock:
            pack_i.value += 1

        with print_lock:
            printn("\r{} - ({}/{}) sequence packets processed ".format(get_work_time(), pack_i.value, packs_at_all))
    #}
    remove_tmp_files("query{}.fasta".format(os.getpid()))
#}


def process_single_file_in_paral(fq_fa_path, i):#{

    # Configure quality dictionary
    qual_dict = configure_qual_dict(fq_fa_path) if is_fastq(fq_fa_path) else None

    # Create the result directory with the name of FASTQ of FASTA file being processed:
    new_dpath = get_curr_res_dir(fq_fa_path, prober_res_dir)

    # Convert FASTQ file to FASTA (if it is FASTQ) and get it's path and number of sequences in it:
    curr_fasta = fastq2fasta(fq_fa_path, new_dpath)

    print("\n {}. '{}' ({} sequences) is processing".format(i+1, os.path.basename(fq_fa_path), curr_fasta["nreads"]))

    # "hname" means human readable name (i.e. without file path and extention)
    fasta_hname = os.path.basename(curr_fasta["fpath"]) # get rid of absolure path
    fasta_hname = re_search(r"(.*)\.(m)?f(ast)?a", fasta_hname).group(1) # get rid of file extention

    # Look around and ckeck if there are results of previous runs of this script
    # If 'look_around' is None -- there is no data from previous run
    previous_data = look_around(new_dpath, curr_fasta["fpath"],
        blast_algorithm)

    if previous_data is None:#{ # If there is no data from previous run

        num_done_reads = 0 # number of successfully processed sequences
        tsv_res_path = "{}_{}_result.tsv".format(os.path.join(new_dpath,
            fasta_hname), blast_algorithm) # form result tsv file path
        # tmp_fpath_template = "{}_{}_temp.txt".format(os.path.join(new_dpath,
        #     fasta_hname), blast_algorithm) # form temporary file path
    #}
    else:#{ # if there is data from previous run

        num_done_reads = previous_data["n_done_reads"] # get number of successfully processed sequences
        packet_size = previous_data["pack_size"] # packet size sholud be the same as it was in previous run
        tsv_res_path = previous_data["tsv_respath"] # result tsv file sholud be the same as during previous run
        # tmp_fpath_template = previous_data["tmp_fpath"] # temporary file sholud be the same as during previous run
    #}

    if num_done_reads > 0:#{
        verb, s_letter = ("have", "s") if num_done_reads != 1 else ("has", "")
        print("{} sequence{} {} been already processed.".format(num_done_reads, s_letter, verb))
    #}
    print("Writing results to file:\n '{}'".format(tsv_res_path))

    how_to_open = OPEN_FUNCS[ is_gzipped(fq_fa_path) ]
    fmt_func = FORMATTING_FUNCS[ is_gzipped(fq_fa_path) ]

    print_lock = mp.Lock()
    write_lock = mp.Lock()
    pack_i = mp.Value('i', 0)
    pack_i_lock = mp.Lock()

    file_part_size = curr_fasta["nreads"] // n_thr
    if curr_fasta["nreads"] // n_thr != 0:
        file_part_size += 1

    packs_at_all = (curr_fasta["nreads"] - num_done_reads) // packet_size # Calculate total number of packets sent from current FASTA file
    if (curr_fasta["nreads"] - num_done_reads) % packet_size != 0: # And this is ceiling (in order not to import 'math')
        packs_at_all += 1

    pool = mp.Pool(n_thr, initializer=init_proc_single_file_in_paral,
        initargs=(print_lock, write_lock, pack_i, pack_i_lock))

    printn("\r{} - (0/{}) sequence packets processed ".format(get_work_time(), packs_at_all))

    pool.starmap(process_part_of_file, [(file_part["fasta"], tsv_res_path, qual_dict, packs_at_all) for file_part in fasta_packets(curr_fasta["fpath"],
        file_part_size, curr_fasta["nreads"], num_done_reads)])
    print() # just print blank line
#}


# =/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=/=
#                       |===== Proceed =====|


# /=== Comments to the kernel loop ===/

# 1. 'curr_fasta' is a dict of the following structure:
#    {
#        "fpath": path_to_FASTA_file (str),
#        "nreads": number_of_reads_in_this_FASTA_file (int)
#    }
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 2. 'previous_data' is a dict of the following structure:
#    {
#        "pack_size": packet_size (int),
#        "tsv_respath": path_to_tsv_file_from_previous_run (str),
#        "n_done_reads": number_of_successfull_requests_from_currenrt_FASTA_file (int),
#        "tmp_fpath": path_to_pemporary_file (str)
#    }
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 3. 'packet' is a dict of the following structure:
#    {
#        "fasta": FASTA_data_containing_query_sequences (str),
#        "names": list_of_sequence_ids_from_FASTA_file (list<str>)
#    }

#                   |===== Kernel loop =====|

print(" - Packet size: {} sequences;".format(packet_size))
print(" - BLAST algorithm: {};\n".format(blast_algorithm))

print(" Following files will be processed:")
for i, path in enumerate(fq_fa_list):#{
    print("   {}. '{}'".format(i+1, path))
#}
if not len(your_own_fasta_lst) == 0:#{
    preposition = "besides" if os.path.exists(acc_fpath) else "instead of"
    print("\n Following FASTA files will be added to database {} downloaded ones:".format(preposition))
    for i, path in enumerate(your_own_fasta_lst):
        print("   {}. '{}'".format(i+1, path))
#}
print('-'*30 + '\n')

# It is a dictionary of accessions and record names.
# Accessions are keys, record names are values.
# This dictionary is filled while processing and at the beginning of continuation.
acc_dict = configure_acc_dict(acc_fpath)

# Build a database
local_fasta = build_local_db(acc_dict, prober_res_dir)


if len(fq_fa_list) >= n_thr:#{
    if n_thr != 1:#{
        print_lock = mp.Lock()
        pool = mp.Pool(n_thr, initializer=init_proc_many_files, initargs=(print_lock,))
        pool.starmap(process_multiple_files, [ (fq_fa_sublist, True) for fq_fa_sublist in spread_files_equally(n_thr, fq_fa_list) ])
    #}
    else:#{
        process_multiple_files(fq_fa_list, parallel=False)
    #}
#}
else:#{

    for i, fq_fa_path in enumerate(fq_fa_list):#{
        process_single_file_in_paral(fq_fa_path, i)
    #}
#}


end_time = time()
print( '\n'+get_work_time() + " ({}) ".format(strftime("%d.%m.%Y %H:%M:%S", localtime(end_time))) + "- Task is completed successfully!\n")
platf_depend_exit(0)